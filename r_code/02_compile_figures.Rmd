---
title: "02_compile_figures"
author: "Bella Oleksy"
date: "created 12/22/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Source libraries & metadata

```{r setup, include=FALSE}


library(here)
# root.dir<-here()
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir='..')
# knitr::opts_knit$set(root.dir='root.dir')
knitr::opts_chunk$set(out.width = '50%',fig.height=10) 
# renv::snapshot()

source(here("r_code/scripts/librariesAndFunctions.R"))

#Read in metadata google sheet
# metadata<-read_sheet("https://docs..google.com/spreadsheets/d/1is87WT3n_TU76pTyiis3Os08JJiSmWc2G6K4bthJhU8/edit#gid=952562522") 
# metadata<-read_csv(here("data/lakeMetadata_20221222.csv")) #time to switch to a static file
metadata<-read_csv(here("data/lakeMetadata_20230126.csv")) #found in error in the Kentucky lake SA
```

# Import data

##### Metabolism

```{r load raw metab & metadata, include=FALSE}
####PULL IN METABOLISM DATA######
source(here("r_code/scripts/dataPullMetab.R")) 

rm(cur)
rm(dontuse)
rm(dontuse2)
rm(MueggelseeZMix)
rm(bella_ER)


#and join

bella_metab_withMetaData <- left_join(bella_metab, metadata, by = c('lakeName'))



##EXTENDED SUMMER -- add a month on either end
# Trim to only "extended" dates. In the northern hemisphere this is May1-Oct1, and in the southern hemisphere this is Nov1-May1
bella_metab_withMetaData_extendedSummer<- bella_metab_withMetaData %>%
  filter(!lakeName=="YunYang") %>% #Exclude YYL because the shoulder seasons are actually the most productive here.
  select(solarDay, lakeName, GPP_mgCm2, GPP_mgCL, zMix, `Latitude (decimal degrees)`) %>%
  mutate(year=year(solarDay),
         DOY=yday(solarDay), 
         hemisphere = ifelse(`Latitude (decimal degrees)` < 0, #Assign northern or southern hemisphere
                        "southern", 
                        "northern")) %>% 
  mutate(summer_start = ifelse(hemisphere=="northern",
                        121, #May 1 - summer start n. hemisphere
                        305), #Nov 1 -  summer start s. hemisphere
         summer_end = ifelse(hemisphere=="northern",
                        274, #Oct 1 - summer end n. hemisphere
                        91)) #April 1- summer end s. hemisphere

#Divide into two dataframes by hemisphere, otherwise couldn't get case_when() to work 
bella_metab_withMetaData_northern_extendedSummer<- bella_metab_withMetaData_extendedSummer %>%
  filter(hemisphere=="northern")%>%
  filter(!lakeName=="YunYang") %>%
  mutate(season = case_when(hemisphere=="northern" & DOY > summer_start & DOY < summer_end ~ "summer",
                            T ~ "other"))  # specify when the "summer" season is in effect in either hemisphere


bella_metab_withMetaData_southern_extendedSummer<- bella_metab_withMetaData_extendedSummer %>%
  filter(hemisphere=="southern")%>%
  mutate(season = case_when(hemisphere=="southern" & DOY > summer_start | DOY < summer_end ~ "summer",
                            T ~ "other"))  # specify when the "summer" season is in effect in either hemisphere

bella_metab_withMetaData_extendedSummer <- bind_rows(bella_metab_withMetaData_northern_extendedSummer, bella_metab_withMetaData_southern_extendedSummer)

bella_metab_extendedSummer<-bella_metab_withMetaData_extendedSummer %>%
  filter(season=="summer") %>%
  select(1:5)

YYL<- bella_metab_withMetaData %>%
  filter(lakeName=="YunYang") %>%
  select(solarDay, lakeName, GPP_mgCm2, GPP_mgCL, zMix)

bella_metab_extendedSummer <- bind_rows(bella_metab_extendedSummer,YYL)


bella_metab_extendedSummer_withMetaData  <- bella_metab_extendedSummer %>%
  mutate(GPP_mgCm2 = na_if(GPP_mgCm2, GPP_mgCm2<1)) %>%
  left_join(., metadata, by = c('lakeName'))

bella_metab_summary <- bella_metab_extendedSummer %>%
  mutate(GPP_mgCm2 = na_if(GPP_mgCm2, GPP_mgCm2<1)) %>%
  group_by(lakeName) %>% 
  summarise(n=n(),
            n_NAs=sum(is.na(GPP_mgCm2)),
            meanGPP= mean(GPP_mgCm2, na.rm = T) , 
            medianGPP = median(GPP_mgCm2, na.rm= T),
            meanzMix=mean(zMix, na.rm=T),
            medianzMix=median(zMix, na.rm=T),
            meanGPP_vol=mean(GPP_mgCL, na.rm=T),
            medianGPP_vol=median(GPP_mgCL, na.rm=T),
            sdGPP= sd(GPP_mgCm2, na.rm = T)) %>% 
  mutate(seGPP=sdGPP/sqrt(n),
         GPP5=meanGPP+qnorm(0.05)*seGPP,
         GPP25=meanGPP+qnorm(0.25)*seGPP,
         GPP50=meanGPP+qnorm(0.5)*seGPP,
         GPP75=meanGPP+qnorm(0.75)*seGPP,
         GPP95=meanGPP+qnorm(0.95)*seGPP,
         perc_missing_days=(n_NAs/153)*100)%>% # calculate percentage of missing days. If > 50%, toss those lakes out
  filter(!perc_missing_days>50) %>%
  mutate(perc_complete=100-perc_missing_days)
```

#### Loads

```{r pull in load estimates, include=FALSE}
#######PULL IN LOAD DATA########

source(here("r_code/scripts/dataPullLoads.R")) #Added on 2021-04-26; Erken added 2021-12-08


#Declutter Environment
rm(list = ls()[grep("mueggelsee", ls())])
rm(list = ls()[grep("_C_", ls())])
rm(list = ls()[grep("_Q_", ls())])
rm(list = ls()[grep("Taupo_", ls())])
rm(list = ls()[grep("loch_", ls())])
rm(list = ls()[grep("zwart_", ls())])
rm(list = ls()[grep("lm", ls())])
rm(list = ls()[grep("Loch", ls())])
rm(list = ls()[grep("erken", ls())])
rm(list = ls()[grep("acton", ls())])
rm(dontuse,  cur)

#All of the MEASURED inflows
glimpse(inflow_conc_summary)

inflow_conc_summary_metab<-inflow_conc_summary %>%
  select(lakeName, contains("DOC"), contains("TP"),dataset) %>%
  # select(-TN_load_kg_total, -TN_load_kgday_mean) %>%
  full_join(.,bella_metab_summary %>%
                  select(lakeName, n, meanGPP, medianGPP,
                         meanGPP_vol, medianGPP_vol,
                         meanzMix, medianzMix),
             by="lakeName") %>%
  rename(
         # DOC_kg_total=DOC_load,
         # TP_kg_total=TP_load,
         DOC_gm3_obs=DOC_gm3,
         TP_mgm3_obs=TP_mgm3) %>%
  select(-c(DOC_load,TP_load)) #We will calculate loads in the same way below
  # mutate(DOC_kg_total=DOC_kg_total/1000, #convert from g/day to kg/day
  #        TP_kg_total=TP_kg_total/1000000, #convert from mg/day to kg/day
  #       DOC_load_kgday_mean=DOC_kg_total/n,
  #        TP_load_kgday_mean=TP_kg_total/n)


#ALl of the MODELED inflows
glimpse(modelled_loads_kg)

#Rename a few columns for less confusion
modelled_loads_kg_new <- modelled_loads_kg %>%
    rename(DOC_gm3_mod=DOC_gm3,
           TP_mgm3_mod=TP_mgm3,
           TP_load_kg_mod=TP_load_kg,
           DOC_log_kg_mod=DOC_load_kg)

#Pull out only the lakes WITHOUT measurements/gauged inflows
# modelled_loads_kg_norepeat<-anti_join(modelled_loads_kg,inflow_conc_summary, by="lakeName") %>%
#   left_join(.,bella_metab_summary %>%
#                   select(lakeName, n, meanGPP, medianGPP,
#                          meanGPP_vol, medianGPP_vol,
#                          meanzMix, medianzMix),
#              by="lakeName") %>%
#   filter(!lakeName=="Rotoiti") %>%
#   mutate(DOC_load_kgday_mean=DOC_load_kg/n,
#          TP_load_kgday_mean=TP_load_kg/n) %>%
#   select(-V, -HRT_days, -Qin) %>%
  # rename(DOC_gm3_mod=DOC_gm3,
  #        TP_mgm3_mod=TP_mgm3)

# loads_master<-left_join(load_comparisons_wide,bella_metab_summary %>%
#                   select(lakeName, n, meanGPP, medianGPP,
#                          meanzMix, medianzMix),
#              by="lakeName") %>%
#   filter(!lakeName=="Rotoiti") %>% #Exclude Rotoiti, high number of missing days
  # mutate(DOC_load_kgday_mean=DOC_load_kg/n,
  #        TP_load_kgday_mean=TP_load_kg/n)


loads_master<-full_join(inflow_conc_summary_metab %>% select(-dataset),
                        modelled_loads_kg_new %>% select(-dataset),
                        by=c("lakeName"))
                             # "meanGPP","medianGPP",
                             # "meanGPP_vol", "medianGPP_vol",
                             # "DOC_load_kg","TP_load_kg",
                             # "DOC_gm3","TP_mgm3",
                             # "DOC_load_kgday_mean","TP_load_kgday_mean","n",
                             # "meanzMix","medianzMix",
                             # "dataset")) 


glimpse(loads_master)

#### MASTER DF! ###
master_df<-right_join(bella_metab_summary %>%
                  select(lakeName, n, meanGPP, medianGPP,
                         meanGPP_vol, medianGPP_vol),
                loads_master, by=c("lakeName","n","medianGPP","meanGPP",
                                   "meanGPP_vol", "medianGPP_vol")) %>%
  # left_join(., metadata %>%
  #                              select(lakeName,`Volume (m3)`, `Lake residence time (year)`), by="lakeName") %>%
  # rename(HRT_days=`Lake residence time (year)`,
  #        V=`Volume (m3)`) %>%
  mutate(
         # HRT_days=HRT_days*365,
         # Qin=V/HRT_days,
         TP_load_measured=TP_mgm3_obs*Qin*(1/1000000),
         DOC_load_measured=DOC_gm3_obs*Qin*(1/1000),
         TP_load_modeled=TP_mgm3_mod*Qin*(1/1000000),
         DOC_load_modeled=DOC_gm3_mod*Qin*(1/1000))


```

#### Lake nutrients

```{r lake nutrient data, include=FALSE}


source(here("r_code/scripts/dataPullNutrients.R"))
rm(list = ls()[grep("nuts", ls())])
rm(list = ls()[grep("UNDERC", ls())])
rm(WATER_CHEM, solomonLakeData, solomonLakeData_trim, dataA, dataB, catchment_newtz)

#Merge with master_df
glimpse(master_df)
glimpse(newts_full)

master_df<- left_join(master_df, newts_full %>%
                                    distinct(lakeName, .keep_all=TRUE) %>%
                                    select(lakeName, TP_ugL, TN_ugL, DOC_mgL,
                                           abs440, abs250, abs280, abs254, chla_ugL),
                      by="lakeName") %>%
  select(-HRT_days, -V, -Qin) %>%
  #Rejoin metadata because something got messed up early on
    left_join(.,metadata %>%
              select(lakeName,  `Surface area (ha)`,
                     `Maximum lake depth (m)`,`Mean lake depth (m)`,
                     `Volume (m3)`,`Lake residence time (year)`,
                     `watershed area (km2)`, `Surface area (km2)`)) %>%
  rename(
         HRT_days=`Lake residence time (year)`,
         SA_ha=`Surface area (ha)`,
         zMax=`Maximum lake depth (m)`,
         zMean=`Mean lake depth (m)`,
         V=`Volume (m3)`,
         SA_km2=`Surface area (km2)`,
         WSA_km2=`watershed area (km2)`) %>%
  mutate(
         HRT_days=HRT_days*365,
         WALA=(WSA_km2-SA_km2)/(SA_km2),
         Qin=V/HRT_days)%>%
  distinct() #remove duplicate rows

```

#### Hydrological classification

```{r}
glimpse(master_df)
master_df <- master_df %>%
  mutate(HRT_sizeclass=
           ifelse(HRT_days >0 & HRT_days <=10, "< 10", 
                  ifelse(HRT_days >10 & HRT_days <=100, "10-100", 
                         ifelse(HRT_days >100 & HRT_days <=1000, "100-1000", 
                                ifelse(HRT_days >1000 & HRT_days <=10000, "1000-10,000",
                                       ifelse(HRT_days >10000 , ">10,000", "error")))))) %>%
  mutate(HRT_sizeclass = factor(HRT_sizeclass,
                                levels = c("< 10", "10-100", "100-1000",
                                           "1000-10,000",">10,000"))) #Reorder factors for plotting
master_df<-master_df %>%
  mutate(LSA_km2=SA_ha/100,
         LSA_sizeclass=
           ifelse(LSA_km2 >0 & LSA_km2 <=0.1, "< 0.1", 
                  ifelse(LSA_km2 >0.1 & LSA_km2 <=1, "0.1-1", 
                         ifelse(LSA_km2 >1 & LSA_km2 <=10, "1-10", 
                                ifelse(LSA_km2 >10 & LSA_km2 <=100, "10-100",
                                       ifelse(LSA_km2 >100 , ">100", "error")))))) %>%
  mutate(LSA_sizeclass = factor(LSA_sizeclass,
                             levels = c("< 0.1", "0.1-1", "1-10",
                                        "10-100",">100"))) #Reorder factors for plotting


master_df<-master_df%>%
  mutate(LSA_sizeclass_quantile=cut_number(LSA_km2, 5, dig.lab=1),
         Lake_maxdepthclass_quantile=cut_number(zMax,5, dig.lab=1),
         Lake_meandepthclass_quantile=cut_number(zMean,5, dig.lab=1),
         ) #Another LSA category type that
#uses evenly spaced quantiles for binning observations. 

#Visualize lake size classes
master_df %>%
  group_by(LSA_sizeclass) %>%
  summarize(n=n()) %>%
  ggplot(aes(x=LSA_sizeclass, y=n, fill=LSA_sizeclass))+
  geom_bar(stat="identity",  color="black")+
  geom_text(aes(label=n), vjust=-1) +
  theme_pubr(base_size = 18)+
  scale_fill_manual(values=c("#003049","#d62828","#f77f00","#fcbf49","#eae2b7"),
                    name="Size class")+
  xlab("Lake surface area size class (km2)")+
  ylab("Count")+
  ylim(c(0,30)) +
  ggtitle("Lake size classes - unequal distribution") 

#Visualize lake HRT classes
master_df %>%
  group_by(HRT_sizeclass) %>%
  summarize(n=n()) %>%
  ggplot(aes(x=HRT_sizeclass, y=n, fill=HRT_sizeclass))+
  geom_bar(stat="identity",  color="black")+
  geom_text(aes(label=n), vjust=-1) +
  theme_pubr(base_size = 18)+
  scale_fill_manual(values=c("#003049","#d62828","#f77f00","#fcbf49","#eae2b7"),
                    name="HRT class")+
  xlab("Lake HRT class (days)")+
  ylab("Count")+
  ylim(c(0,30)) +
  ggtitle("Lake HRT classes - unequal distribution") 

```

#Potential figures

### Figure 3 - correleations among GPP, loads, zmix, nutrients, SA, volume

```{r}


## First version uses OBSERVED GPP, and the only modeled data is TPin and DOCin
my_labeller <- as_labeller(c(HRT_days="HRT~(days)", LSA_km2="Surface~Area~(km^2)",
                             meanzMix="Mixed~layer~depth~(m)", V="Volume~(~10^6~m^3)",
                             DOC_mgL="DOC[lake]~(g~m^-3)", TP_ugL="TP[lake]~(mg~m^-3)",
                             WALA="WSA:LA",
                             # DOC_load_modeled="Mean~daily~DOC~load~(kg)",
                             # TP_load_modeled="Mean~daily~TP~load~(kg)",
                             TP_mgm3_obs="TP[inflow]~(mg~m^-3)",
                             TP_mgm3_mod="TP[inflow]~(mu~g~L^-1)",
                             DOC_gm3_mod="DOC[inflow]~(mg~L^1)",
                             CP="DOC~to~TP~ratio[lake]"),
                           default = label_parsed)


linear_models <- master_df %>%
  select(medianGPP, HRT_days, WALA,
         meanzMix, TP_ugL, DOC_mgL, LSA_km2, V) %>%
  mutate(CP = DOC_mgL / (TP_ugL),
         V = V / 1000000) %>%
  pivot_longer(-(1)) %>%
  rename(predictor = name) %>%
  nest(data = -c(predictor)) %>%
  mutate(
    fit = map(data, ~glm(log(medianGPP) ~ log(value), data = .x, family = Gamma(link = "log")))
  ) %>%
  mutate(
    tidy_out = map(fit, broom::tidy),
    glance_out = map(fit, broom::glance)
  ) %>%
  unnest(c(tidy_out, glance_out)) %>%
  select(-fit, -data) %>%
  filter(term != "(Intercept)") %>%
  mutate(p.value = round(p.value, 3),
         p.value = p.adjust(p.value, method = "bonferroni"),
         p.value = replace(p.value, p.value < 0.001, "<0.001"),
         x = "left",
         y = "top") %>%
  mutate(predictor = factor(predictor,
                             levels = c("DOC_mgL",
                                        "TP_ugL",
                                        "CP", "meanzMix",
                                        "HRT_days", "WALA",
                                        "LSA_km2", "V")))


GPP_correlations<-master_df %>%
  select(lakeName, medianGPP, HRT_days, meanzMix, TP_ugL, DOC_mgL, LSA_km2, V, WALA) %>%
  mutate(CP=DOC_mgL/(TP_ugL),
         V=V/1000000) %>%
  pivot_longer(-(1:2)) %>%
  rename(predictor=name)%>%
  mutate(predictor=factor(predictor,
                     levels=c("DOC_mgL",
                              "TP_ugL",
                              "CP","meanzMix",
                              "HRT_days","WALA",
                              "LSA_km2","V")),
         dataset = case_when(lakeName %in% observed_loads_kelly$lakeName ~ "calibration",
                             TRUE ~ "modeled"))%>%
  ggplot(aes(x=value, y=medianGPP, group=predictor))+
  geom_smooth(method="lm", se=T, color="grey50", fill="black", linewidth=0.5, fullrange=TRUE)+
  geom_point(aes(shape=dataset), size=3,color="black", fill="black", alpha=0.5)+
  facet_wrap(.~predictor, scales="free",ncol=2,
             labeller=my_labeller, strip.position="bottom")+
  scale_y_log10(expression(paste('Median observed GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma, breaks_log(n = 6, base = 10))+
  scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
    scale_shape_manual(values=c(21,25),
                     name="Dataset:")+
  annotation_logticks()+
  theme_few()+
  theme(
        legend.position="bottom",
        legend.text.align = 0.5,
        legend.margin = margin(0, 0, 0, 0),
        axis.title.x=element_blank(),
        strip.placement = "outside",
        strip.background = element_blank(),
        plot.margin = margin(1,1,0.5,0.5, "cm") 
        # strip.text = element_text(size=12),
        # axis.text.y= element_text(size=12),
        )+
  ggpp::geom_label_npc(data = linear_models,
                 aes(npcx = x, npcy = y, label = paste("p=",p.value)),
                 size=3, alpha=0.5)

GPP_correlations  
ggsave(here("figures/MS/Figure1.GPP_correlations.pdf"), dpi=600, width=6,height=8, units="in")


### ALL THE OTHER PLOTS BELOW THIS ARE EXPLORATORY AND DON'T NEED TO BE RUN 

## This version uses OBSERVED GPP, and the only modeled data is TPin and DOCin
# my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
#                              meanzMix="log[10]~Mixed~layer~depth~(m)", V="log[10]~Volume~(~10^6~m^3)",
#                              DOC_mgL="DOC[lake]~(mg~L^1)", TP_ugL="TP[lake]~(mu~g~L^-1)",
#                              # DOC_load_modeled="Mean~daily~DOC~load~(kg)",
#                              # TP_load_modeled="Mean~daily~TP~load~(kg)",
#                              TP_mgm3_mod="TP[inflow]~(mu~g~L^-1)",
#                              DOC_gm3_mod="DOC[inflow]~(mg~L^1)",
#                              CP="DOC~to~TP~ratio[inflow]"),
#                              
#                            default = label_parsed)
# 
# GPP_correlations<-master_df %>%
#   select(meanGPP, DOC_gm3_mod, TP_mgm3_mod,
#          meanzMix, TP_ugL, DOC_mgL, LSA_km2, V) %>%
#   mutate(CP=DOC_gm3_mod/(TP_mgm3_mod),
#          V=V/1000000) %>%
#   pivot_longer(-(1)) %>%
#   mutate(name=factor(name,
#                      levels=c("DOC_gm3_mod","DOC_mgL",
#                               "TP_mgm3_mod","TP_ugL",
#                               "CP","meanzMix",
#                               "LSA_km2","V")))%>%
#   ggplot(aes(x=value, y=meanGPP))+
#   geom_smooth(method="lm", se=T, color="grey50", fill="black", size=0.5, alpha=0.5)+
#   geom_point(size=3, shape=21, color="black", fill="black", alpha=0.5)+
#   facet_wrap(.~name, scales="free",ncol=2,
#              labeller=my_labeller, strip.position="bottom")+
#   scale_y_log10(expression(paste('Observed GPP (mg C m'^-2,' day'^-1,')')),
#                 labels = scales::comma, breaks_log(n = 6, base = 10))+
#   scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
#   theme_few()+
#   theme(legend.position="none",
#         axis.title.x=element_blank(),
#         strip.placement = "outside",
#         strip.background = element_blank(),
#         plot.margin = margin(1,1,0.5,0.5, "cm") 
#         # strip.text = element_text(size=12),
#         # axis.text.y= element_text(size=12),
#         )
# 
# GPP_correlations  
# ggsave(here("figures/GPP_correlations_original.png"), dpi=600, width=6,height=8, units="in")
# 
# 
# 
# 
# 
# ## This version uses only PREDICTED values. 
# ## Are the patterns qualitatively the same as above? 
# preds_full <-read.csv(here('data/preliminaryMetropolisResults/ExtendedRangeParameter_outputWestimatedloads.csv'), header=T,sep=",") 
# names(preds_full)
# 
# 
# 
# my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
#                              zMixHat="log[10]~Mixed~layer~depth~(m)", V="log[10]~Volume~(~10^6~m^3)",
#                              CHat="DOC[lake]~(g~m^-3)", TPHat="TP[lake]~(mg~m^-3)",
#                              # DOCin="Mean~daily~DOC~load~(kg)",
#                              # Pin="Mean~daily~TP~load~(kg)",
#                              Pin="TP[inflow]~(mu~g~L^-1)",
#                              DOCin="DOC[inflow]~(mg~L^1)",
#                              CP="DOC~to~TP~ratio~(inflow)"),
#                              default = label_parsed)
# 
# predGPP_correlations<-
#   preds_full %>%
#   left_join(., master_df %>%
#               select(lakeName, LSA_km2, V), by="lakeName") %>%
#   mutate(CP=DOCin/(Pin),
#          V=V/1000000) %>%
#   select(-c(AHat, PHat)) %>%
#   pivot_longer(-(c("lakeName","GPPHat"))) %>%
#   mutate(name=factor(name,
#                      levels=c("GPPHat","DOCin","CHat",
#                               "Pin","TPHat",
#                               "CP","zMixHat",
#                               "LSA_km2","V")))%>%
#   ggplot(aes(x=value, y=GPPHat))+
#   geom_smooth(method="lm", se=T, color="grey50", fill="#fee08b", size=0.5)+
#   geom_point(size=3, shape=21, color="black", fill="#fee08b", alpha=0.5)+
#   facet_wrap(.~name, scales="free",ncol=2,
#              labeller=my_labeller, strip.position="bottom")+
#   scale_y_log10(expression(paste('Predicted GPP (mg C m'^-2,' day'^-1,')')),
#                 labels = scales::comma, breaks_log(n = 6, base = 10))+
#   scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
#   theme_few()+
#   theme(legend.position="none",
#         axis.title.x=element_blank(),
#         strip.placement = "outside",
#         strip.background = element_blank(),
#         plot.margin = margin(1,1,0.5,0.5, "cm") 
#         # strip.text = element_text(size=12),
#         # axis.text.y= element_text(size=12),
#         )
# 
# predGPP_correlations  
# ggsave(here("figures/GPP_correlations_predictedVals.png"), dpi=600, width=6,height=8, units="in")
# 
# 
# #####Both on the same graph
# my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
#                              meanzMix="log[10]~Mixed~layer~depth~(m)", V="log[10]~Volume~(~10^6~m^3)",
#                              DOC_mgL="DOC~(mg~L^1)", TP_ugL="TP~(mu~g~L^-1)",
#                              TP_mgm3_mod="TP[inflow]~(mu~g~L^-1)",
#                              DOC_gm3_mod="DOC[inflow]~(mg~L^1)",
#                              CP="DOC~to~TP~ratio~(loads)"),
#                              
#                            default = label_parsed)
# 
# 
#   plot_data<-  master_df %>%
#   left_join(., preds_full %>%
#                 select(lakeName, GPPHat),  by="lakeName") %>%
#   select(meanGPP, GPPHat, DOC_gm3_mod, TP_mgm3_mod,
#          meanzMix, TP_ugL, DOC_mgL, LSA_km2, V) %>%
#   mutate(CP=DOC_gm3_mod/(TP_mgm3_mod),
#          V=V/1000000) %>%
#   pivot_longer(-(1:2)) %>%
#   rename(predictor_variable=name,
#          predictor_value=value)%>%
#   mutate(predictor_variable=factor(predictor_variable,
#                      levels=c("DOC_gm3_mod","DOC_mgL",
#                               "TP_mgm3_mod","TP_ugL",
#                               "CP","meanzMix",
#                               "LSA_km2","V")))%>%
#   pivot_longer(-(3:4)) %>%
#   rename(response_variable=name,
#          response_value=value)%>%
#   # filter(response_value>1)%>%
#   mutate(response_variable=factor(response_variable,
#                      labels=c("Measured GPP","Modeled GPP")))
# 
# predGPP_correlations<- plot_data %>%
#   # filter(response_variable=="Modeled GPP")%>%
#   ggplot(aes(x=predictor_value, y=response_value, fill=response_variable, linetype=response_variable))+
#   geom_point(size=3, shape=21, color="black", alpha=0.5)+
#   geom_smooth(method="lm", se=T, color="black")+
#   facet_wrap(.~predictor_variable, scales="free",ncol=2,
#              labeller=my_labeller, strip.position="bottom")+
#   scale_y_log10(expression(paste('GPP (mg C m'^-2,' day'^-1,')')),
#                 labels = scales::comma, breaks_log(n = 6, base = 10))+
#   scale_fill_manual(values=c("#ffc300","#003566"),
#                     name="Legend:")+
#   scale_linetype_manual(values=c(1,6),
#                     name="Legend:")+
#   scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
#   theme_pubr(base_size=10,
#              border=TRUE)+
#   theme(axis.title.x=element_blank(),
#         strip.placement = "outside",
#         strip.background = element_blank(),
#         plot.margin = margin(1,1,0.5,0.5, "cm") 
#         # strip.text = element_text(size=12),
#         # axis.text.y= element_text(size=12),
#         )
# 
# predGPP_correlations  
# 
# ggsave(here("figures/GPP_correlations_obs_and_predicted_Vals.png"), dpi=600, width=8,height=8, units="in")
# 
# 
# 
# 
# ##### Same as above but with inflow loads instead of concentrations
# my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
#                              meanzMix="log[10]~Mixed~layer~depth~(m)", V="log[10]~Volume~(~10^6~m^3)",
#                              DOC_mgL="DOC~(mg~L^1)", TP_ugL="TP~(mu~g~L^-1)",
#                              TP_load_modeled="Mean~daily~TP~load~(kg)",
#                              DOC_load_modeled="Mean~daily~DOC~load~(kg)",
#                              CP="DOC~to~TP~ratio~(loads)"),
#                              
#                            default = label_parsed)
# 
# 
#   plot_data<-  master_df %>%
#   left_join(., preds_full %>%
#                 select(lakeName, GPPHat),  by="lakeName") %>%
#   select(meanGPP, GPPHat, DOC_load_modeled, TP_load_modeled,
#          meanzMix, TP_ugL, DOC_mgL, LSA_km2, V) %>%
#   mutate(CP=DOC_load_modeled/(TP_load_modeled),
#          V=V/1000000) %>%
#   pivot_longer(-(1:2)) %>%
#   rename(predictor_variable=name,
#          predictor_value=value)%>%
#   mutate(predictor_variable=factor(predictor_variable,
#                      levels=c("DOC_load_modeled","DOC_mgL",
#                               "TP_load_modeled","TP_ugL",
#                               "CP","meanzMix",
#                               "LSA_km2","V")))%>%
#   pivot_longer(-(3:4)) %>%
#   rename(response_variable=name,
#          response_value=value)%>%
#   # filter(response_value>1)%>%
#   mutate(response_variable=factor(response_variable,
#                      labels=c("Measured GPP","Modeled GPP")))
# 
# predGPP_correlations2<- plot_data %>%
#   # filter(response_variable=="Modeled GPP")%>%
#   ggplot(aes(x=predictor_value, y=response_value, fill=response_variable, linetype=response_variable))+
#   geom_point(size=3, shape=21, color="black", alpha=0.5)+
#   geom_smooth(method="lm", se=T, color="black")+
#   facet_wrap(.~predictor_variable, scales="free",ncol=2,
#              labeller=my_labeller, strip.position="bottom")+
#   scale_y_log10(expression(paste('GPP (mg C m'^-2,' day'^-1,')')),
#                 labels = scales::comma, breaks_log(n = 6, base = 10))+
#   scale_fill_manual(values=c("#ffc300","#003566"),
#                     name="Legend:")+
#   scale_linetype_manual(values=c(1,6),
#                     name="Legend:")+
#   scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
#   theme_classic()+
#   theme(axis.title.x=element_blank(),
#         strip.placement = "outside",
#         strip.background = element_blank(),
#         plot.margin = margin(1,1,0.5,0.5, "cm") 
#         # strip.text = element_text(size=12),
#         # axis.text.y= element_text(size=12),
#         )
# 
# predGPP_correlations2  
# 
# ggsave(here("figures/GPP_correlations_obs_and_predicted_Vals_v2.png"), dpi=600, width=6,height=8, units="in")



```


### Figure 4, Figure S3

```{r}

predVals<-read.csv(here("data/metropolis_results/kellyModelOutput_wObservedLoads_2023-12-06.csv")) 
obsVals<-read.csv(here("data/export/master_df_export.csv")) %>% select(-X)
predVals_outOfSample<-read.csv(here("data/metropolis_results/kellyModelOutput_wModeledLoads_2023-12-06.csv")) 
# obsVals_outOfSample<-read.csv(here("data/metropolis_results/gridSearchInput_unmodeledInfoLakes4bella.csv")) 

compareVals<-left_join(predVals,obsVals %>%
                         select(lakeName, TP, DOC, meanzMix, medianGPP, meanGPP)) %>%
  mutate(set="calibration")
compareVals2 <- left_join(predVals_outOfSample,obsVals %>%
                            select(lakeName, TP, DOC, meanzMix, medianGPP, meanGPP)) %>%
  mutate(set="modeled") %>%
  filter(!lakeName %in% compareVals$lakeName)

all_compareVals <- bind_rows(compareVals,
                             compareVals2)


#How do DOClake and DOCin vary? Same for TP?
TPlakevsTPin<-  all_compareVals %>%
  # filter(!lakeName=="Annie")%>%
  ggplot(aes(x=TP, y=Pin, shape=set))+
  geom_point(fill="black", alpha=0.5, size=2.2)+
  scale_shape_manual(values=c(21,25),
                     name="Dataset:")+
  theme_pubr(base_size = 12, border=TRUE)+
  geom_abline(slope=1,intercept=0)+
  xlab(expression(paste('Lake TP (mg m'^-3,')')))+
  ylab(expression(paste('Inflow TP (mg m'^-3,')')))

DOClakevsDOCin <- all_compareVals %>%
  # filter(!lakeName=="Annie")%>%
  ggplot(aes(x=DOC, y=DOCin, shape=set))+
  geom_point(fill="black", alpha=0.5, size=2.2)+
  scale_shape_manual(values=c(21,25),
                     name="Dataset:")+
  theme_pubr(base_size = 12, border=TRUE)+
  geom_abline(slope=1,intercept=0)+
  xlab(expression(paste('Lake DOC (mg m'^-3,')')))+
  ylab(expression(paste('Inflow DOC (mg m'^-3,')')))


combined <- TPlakevsTPin + DOClakevsDOCin &   theme(legend.position="bottom",
        legend.text.align = 0.5,
        legend.margin = margin(0, 0, 0, 0))
combined +  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")
ggsave(here("figures/MS/FigureS3.DOC_or_TP_lake_vs_inflow.png"), width=8, height=4,units="in", dpi=600)




### In this version, the fill corresponds to DOC:TP
#This version makes sure to use the actual DOCin and TPin conc for the 18 lakes, not modeled

my_breaks_x = c(0,25,50,75,100,125)
my_breaks_y = c(0,2500,5000,7500)

GPP_DOC_lake_DOCTP <-
  all_compareVals %>%
  left_join(., master_df %>% select(lakeName, LSA_sizeclass)) %>%
  ggplot(aes(y=(medianGPP), x=DOC,
             fill=(DOCin/Pin)))+
  geom_point(color="black", alpha=0.9, size=2.5,
             shape=21)+
  scale_fill_distiller(palette ="YlOrRd", direction=-1,
                       name="Inflow DOC:TP", trans="log10",
                       guide = guide_colorbar(frame.colour = "black", ticks.colour = "black"))+
  theme_pubr(base_size = 12, border=TRUE)+
  scale_x_continuous(name=expression(paste('Lake DOC (g m'^-3,')')),
                     breaks=my_breaks_x,
                     labels=my_breaks_x,
                     limits=c(0,130))+
  scale_y_continuous(name=expression(paste('Median GPP (mg C m'^-2,' day'^-1,')')),
                     breaks=my_breaks_y,
                     labels=my_breaks_y,
                     limits=c(0,9000))+
  theme(legend.position="bottom")

#TP lake vs DOC lake
my_breaks_x = c(0,20,40,60)
my_breaks_y = c(0,100,200,300)

TP_DOC_lake_GPP <-  all_compareVals %>%
  left_join(., master_df %>% select(lakeName, LSA_sizeclass)) %>%
  ggplot(aes(y=TP, x=DOC,
             fill=(medianGPP)))+
  geom_point(color="black", alpha=0.9, size=2.5,
             shape=21)+
  scale_fill_distiller(palette ="YlGnBu", direction=1,
                       name="Obs. GPP", trans = "log10",
                       guide = guide_colorbar(frame.colour = "black",
                                              ticks.colour = "black"))+
  theme_pubr(base_size = 12, border=TRUE)+ 
  scale_y_continuous(name=expression(paste('Lake TP (mg m'^-3,')')),
                     breaks=my_breaks_y,
                     labels=my_breaks_y,
                     limits=c(0,300))+
  scale_x_continuous(name=expression(paste('Lake DOC (g m'^-3,')')),
                     breaks=my_breaks_x,
                     labels=my_breaks_x,
                     limits=c(0,60))+
  theme(legend.position="bottom")

combined <- GPP_DOC_lake_DOCTP + TP_DOC_lake_GPP &   theme(legend.position="bottom",
        legend.text.align = 0.5,
        legend.margin = margin(0, 0, 0, 0),
        plot.margin=margin(1,3,1,1),
        legend.key.width  = unit(2, "lines"),
        legend.key.height = unit(1, "lines"),
        legend.text = element_text(size=11))
combined +  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")
  
ggsave(here("figures/MS/Figure4_twopanel.png"), width=8, height=4,units="in", dpi=600)

```


### Figure 3. Pred vs Obs params for 18 calibration lakes

```{r}

predVals<-read.csv(here("data/metropolis_results/kellyModelOutput_wObservedLoads_2023-03-17.csv")) 
obsVals<-read.csv(here("data/export/master_df_export.csv")) %>% select(-X)
predVals_outOfSample<-read.csv(here("data/metropolis_results/kellyModelOutput_wModeledLoads_2023-03-17.csv")) 
# obsVals_outOfSample<-read.csv(here("data/metropolis_results/gridSearchInput_unmodeledInfoLakes4bella.csv")) 

compareVals<-left_join(predVals,obsVals %>%
                         select(lakeName, TP, DOC, meanzMix, medianGPP, meanGPP)) %>%
  mutate(set="calibration")
compareVals2 <- left_join(predVals_outOfSample,obsVals %>%
                            select(lakeName, TP, DOC, meanzMix, medianGPP, meanGPP)) %>%
  mutate(set="modeled") %>%
  filter(!lakeName %in% compareVals$lakeName)

all_compareVals <- bind_rows(compareVals,
                             compareVals2)

#Test whether slopes are different from 0
fitTP <- lm(TPHat~TP, compareVals)
summary(fitTP)
confint(fitTP)
TP.emt <- emtrends(fitTP, ~1, var="TP")
TP.emt
test(TP.emt, null=1)


fitDOC <- lm(CHat~DOC, compareVals)
summary(fitDOC)
confint(fitDOC)
DOC.emt <- emtrends(fitDOC, ~1, var="DOC")
DOC.emt
test(DOC.emt, null=1)


fitzMix <- lm(zMixHat~meanzMix, compareVals)
summary(fitzMix)
confint(fitzMix)
zMix.emt <- emtrends(fitzMix, ~1, var="meanzMix")
zMix.emt
test(zMix.emt, null=1)


fitGPPmean <- lm(GPPHat~meanGPP, compareVals)
fitGPPmean
GPP.mean.emt <- emtrends(fitGPPmean, ~1, var="meanGPP")
GPP.mean.emt
test(GPP.mean.emt, null=1)

fitGPPmed <- lm(GPPHat~medianGPP, compareVals)
summary(fitGPPmed)
confint(fitGPPmed)
GPP.med.emt <- emtrends(fitGPPmed, ~1, var="medianGPP")
GPP.med.emt
test(GPP.med.emt, null=1)



## RMSEs
GPP_vals <- compareVals %>% select(GPPHat, meanGPP, medianGPP)

#MeanGPP
actual_GPP_mean <- GPP_vals$meanGPP
#MedianGPP
actual_GPP_median <- GPP_vals$medianGPP

pred_GPP <- GPP_vals$GPPHat

#RMSE with MeanGPP
Metrics::rmse(actual_GPP_mean,pred_GPP)

#RMSE with MedianGPP
Metrics::rmse(actual_GPP_median,pred_GPP)
Metrics::smape(actual_GPP_median,pred_GPP) #Symmetiric Mean Absolute Percent Error
#difference between smape and mape: https://towardsdatascience.com/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac

#RMSE with TP
Metrics::rmse(compareVals$TPHat,compareVals$TP)
Metrics::smape(compareVals$TPHat,compareVals$TP) #Mean Absolute Percent Error

#RMSE with DOC
Metrics::rmse(compareVals$CHat,compareVals$DOC)
Metrics::smape(compareVals$CHat,compareVals$DOC) #Mean Absolute Percent Error

#RMSE with zMix
Metrics::rmse(compareVals$zMixHat,compareVals$meanzMix)
Metrics::smape(compareVals$zMixHat,compareVals$meanzMix) #Mean Absolute Percent Error


#A better way, probably
tidy2 <- function(mod) {
  tidy(mod, conf.int = TRUE, conf.level = 0.95)
}


slopes <- compareVals %>%
  select(GPPHat,medianGPP) %>%
  rename(pred=GPPHat,
         obs=medianGPP)%>%
  mutate(name="GPP") %>%
  bind_rows(., compareVals %>%
  select(zMixHat,meanzMix) %>%
  rename(pred=zMixHat,
         obs=meanzMix)%>%
  mutate(name="zMix")) %>%
  bind_rows(., compareVals %>%
  select(TPHat,TP) %>%
  rename(pred=TPHat,
         obs=TP)%>%
  mutate(name="TP")) %>%
  bind_rows(., compareVals %>%
  select(CHat,DOC) %>%
  rename(pred=CHat,
         obs=DOC)%>%
  mutate(name="DOC")) %>%
  nest(data = -c(name)) %>%
  mutate(
    fit = map(data, ~lm(pred ~ obs, data = .x)),
    tidy_out = map(fit, tidy2)
  ) %>%
  unnest(cols = tidy_out) %>%
  select(-fit, -data) %>%
  filter(term != "(Intercept)") %>%
  # filter(p.value<0.05) %>%
  mutate(p.value=round(p.value,3),
         conf.low=round(conf.low,2),
         conf.high=round(conf.high,2),
         estimate=round(estimate,1),
         x="left",
         y="top")


## This version includes only the calibration set (18 lakes)
my_breaks = c(3,10,30,100)
A <- compareVals %>%
  ggplot(aes(y=TPHat, x=TP))+
  geom_point(alpha=0.7, size=2.5, shape=21, fill="#E69F00")+
  geom_abline(intercept=0, slope=1)+
  geom_smooth(method="lm",se=F,color="#E69F00",linewidth=0.5)+
  scale_x_log10(name = expression(paste('Mean lake TP (mg m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(3,100))+
  scale_y_log10(name = expression(paste('Modeled lake TP (mg m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(3,100))+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  ggpp::geom_label_npc(data = slopes %>% filter(name=="TP"),
                       fill="#E69F00",
                 aes(npcx = x, npcy = y, label = paste("SMAPE = ",
                                                       round(Metrics::smape(compareVals$TPHat,compareVals$TP),2)*100,"%",
                                                       "\nRMSE = ",round(Metrics::rmse(compareVals$TPHat,compareVals$TP),1),"\nslope = ",estimate," (",conf.low,",",conf.high,")", sep="")),
                 size=3, alpha=0.5)



my_breaks = c(0.3,1,3,10,30)
B  <- compareVals %>%
  ggplot(aes(y=CHat, x=DOC))+
  geom_point(alpha=0.7, size=2.5, shape=21, fill="#E69F00")+
  geom_abline(intercept=0, slope=1)+
  geom_smooth(method="lm",se=F,color="#E69F00",linewidth=0.5)+
  scale_x_log10(name = expression(paste('Mean lake DOC (g m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(0.3,30))+
  scale_y_log10(name = expression(paste('Modeled lake DOC (g m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(0.3,30))+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  ggpp::geom_label_npc(data = slopes %>% filter(name=="DOC"),
                       fill="#E69F00",
                 aes(npcx = x, npcy = y, label = paste("SMAPE = ",
                                                       round(Metrics::smape(compareVals$CHat,compareVals$DOC),2)*100,"%",
                                                       "\nRMSE = ",round(Metrics::rmse(compareVals$CHat,compareVals$DOC),1),"\nslope = ",estimate," (",conf.low,",",conf.high,")", sep="")),
                 size=3, alpha=0.5)

my_breaks = c(1,3,10,30)
C  <- compareVals %>%
  ggplot(aes(y=zMixHat, x=meanzMix))+
  geom_point(alpha=0.7, size=2.5, shape=21, fill="#E69F00")+
  geom_abline(intercept=0, slope=1)+
  geom_smooth(method="lm",se=F,color="#E69F00",linewidth=0.5)+
  scale_x_log10(name = expression(paste('Mean z'[mix],' (m)')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(1,30))+
  scale_y_log10(name = expression(paste('Modeled z'[mix],' (m)')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(1,30))+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  ggpp::geom_label_npc(data = slopes %>% filter(name=="zMix"),
                       fill="#E69F00",
                 aes(npcx = x, npcy = y, label = paste("SMAPE = ",
                                                       round(Metrics::smape(compareVals$zMixHat,compareVals$meanzMix),2)*100,"%",
                                                       "\nRMSE = ",round(Metrics::rmse(compareVals$zMixHat,compareVals$meanzMix),1),"\nslope = ",estimate," (",conf.low,",",conf.high,")", sep="")),
                 size=3, alpha=0.5)

my_breaks = c(30,100,300,1000,3000,10000)
D  <-
  compareVals %>%
  ggplot(aes(y=GPPHat, x=medianGPP))+
  geom_point(alpha=0.7, size=2.5, shape=21, fill="#E69F00")+
  geom_abline(intercept=0, slope=1)+
  geom_smooth(method="lm",se=F,color="#E69F00",linewidth=0.5)+  scale_x_log10(name = expression(paste('Mean GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma,
                breaks = my_breaks,
                limits = c(30,10000)
                )+
  scale_y_log10(name = expression(paste('Modeled GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma,
                breaks = my_breaks,
                limits = c(30,10000)
                )+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  ggpp::geom_label_npc(data = slopes %>% filter(name=="GPP"),
                       fill="#E69F00",
                 aes(npcx = x, npcy = y, label = paste("SMAPE = ",
                                                       round(Metrics::smape(compareVals$GPPHat,compareVals$medianGPP),2)*100,"%",
                                                       "\nRMSE = ",round(Metrics::rmse(compareVals$GPPHat,compareVals$medianGPP),1),"\nslope = ",estimate," (",conf.low,",",conf.high,")", sep="")),
                 size=3, alpha=0.5)


(A+B)/(C+D) + plot_annotation(tag_levels = 'A')

ggsave(here("figures/MS/FigS3.modeled_vs_measured_TP_DOC_zMIX_GPP_18lakes.png"), dpi=600, width=8,height=8, units="in")



## This version includes all of the lakes 
## RMSEs

test_set_vals <- all_compareVals %>% filter(set=="modeled") %>% select(GPPHat, meanGPP, medianGPP, meanzMix, zMixHat,
                                                                       TPHat, TP, CHat, DOC)

#MeanGPP
actual_GPP_mean <- test_set_vals$meanGPP
#MedianGPP
actual_GPP_median <- test_set_vals$medianGPP
pred_GPP <- test_set_vals$GPPHat
#zMix
actual_zmix <- test_set_vals$meanzMix
pred_zmix <- test_set_vals$zMixHat

#RMSE of MeanGPP
Metrics::rmse(actual_GPP_mean,pred_GPP)

#RMSE of MedianGPP
Metrics::rmse(actual_GPP_median,pred_GPP)
Metrics::smape(actual_GPP_median,pred_GPP) #Symmetiric Mean Absolute Percent Error


#RMSE of zmix
Metrics::rmse(actual_zmix,pred_zmix)
Metrics::smape(actual_zmix,pred_zmix)


fitzMix <- lm(zMixHat~meanzMix, test_set_vals)
summary(fitzMix)
confint(fitzMix)

zMix.emt <- emtrends(fitzMix, ~1, var="meanzMix")
zMix.emt
test(zMix.emt, null=1)


fitGPPmean <- lm(GPPHat~meanGPP, test_set_vals)
fitGPPmean
GPP.mean.emt <- emtrends(fitGPPmean, ~1, var="meanGPP")
GPP.mean.emt
test(GPP.mean.emt, null=1)

fitGPPmed <- lm(GPPHat~medianGPP, test_set_vals)
summary(fitGPPmed)
confint(fitGPPmed)
GPP.med.emt <- emtrends(fitGPPmed, ~1, var="medianGPP")
GPP.med.emt
test(GPP.med.emt, null=1)
tidy2(fitGPPmed)

#A better way, probably
tidy2 <- function(mod) {
  tidy(mod, conf.int = TRUE, conf.level = 0.95)
}


slopes <- test_set_vals %>%
  select(GPPHat,medianGPP) %>%
  rename(pred=GPPHat,
         obs=medianGPP)%>%
  mutate(name="GPP") %>%
  bind_rows(., test_set_vals %>%
  select(zMixHat,meanzMix) %>%
  rename(pred=zMixHat,
         obs=meanzMix)%>%
  mutate(name="zMix")) %>%
  nest(data = -c(name)) %>%
  mutate(
    fit = map(data, ~lm(pred ~ obs, data = .x)),
    tidy_out = map(fit, tidy2)
  ) %>%
  unnest(cols = tidy_out) %>%
  select(-fit, -data) %>%
  filter(term != "(Intercept)") %>%
  # filter(p.value<0.05) %>%
  mutate(p.value=round(p.value,3),
         conf.low=round(conf.low,2),
         conf.high=round(conf.high,2),
         estimate=round(estimate,1),
         x="left",
         y="top")


my_breaks = c(3,10,30,100)
A <- all_compareVals %>%
  ggplot(aes(y=TPHat, x=TP, fill=set))+
  geom_point(alpha=0.7, size=2.5, shape=21)+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  colorblindr::scale_fill_OkabeIto(name="Lake set:")+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean lake TP (mg m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(3,100))+
  scale_y_log10(name = expression(paste('Modeled lake TP (mg m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(3,100))+
  guides(fill = guide_legend(show = FALSE),
         line = guide_legend(show = FALSE))


my_breaks = c(0.3,1,3,10,30)
B  <- all_compareVals %>%
  ggplot(aes(y=CHat, x=DOC, fill=set))+
  geom_point(alpha=0.7, size=2.5, shape=21)+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  colorblindr::scale_fill_OkabeIto(name="Lake set:")+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean lake DOC (g m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(0.3,30))+
  scale_y_log10(name = expression(paste('Modeled lake DOC (g m'^-3,')')),
                labels = my_breaks,
                breaks = my_breaks,
                limits = c(0.3,30))+
  guides(fill = guide_legend(show = FALSE),
         line = guide_legend(show = FALSE))


my_breaks = c(1,3,10,30)
C  <- all_compareVals %>%
  ggplot(aes(y=zMixHat, x=meanzMix,
             fill=set, color=set))+
  geom_point(alpha=0.7, size=2.5, shape=21, color="black")+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  colorblindr::scale_fill_OkabeIto(name="Lake set:")+
  colorblindr::scale_color_OkabeIto(name="Lake set:")+
  geom_smooth(method="lm", se=FALSE, show.legend=FALSE)+
  geom_abline(intercept=0, slope=1)+
  # scale_x_log10(name = expression(paste('Mean z'[mix],' (m)')),
  #               labels = my_breaks,
  #               breaks = my_breaks,
  #               limits = c(1,30))+
  # scale_y_log10(name = expression(paste('Modeled z'[mix],' (m)')),
  #               labels = my_breaks,
  #               breaks = my_breaks,
  #               limits = c(1,30))+
  ggpp::geom_label_npc(data = slopes %>% filter(name=="zMix"),
                       fill="#56B4E9",
                 aes(npcx = x, npcy = y, label = paste("SMAPE = ",
                                                       round(Metrics::smape(actual_zmix,pred_zmix),2),"%",
                                                       "\nRMSE = ",round(Metrics::rmse(actual_zmix,pred_zmix),1),"\nslope = ",estimate," (",conf.low,",",conf.high,")", sep="")),
                 size=3, alpha=0.5)

my_breaks = c(30,100,300,1000,3000,10000)
D  <-
  all_compareVals %>%
  ggplot(aes(y=GPPHat, x=medianGPP, fill=set, color=set))+
  geom_point(alpha=0.7, size=2.5, shape=21, color="black")+
  geom_smooth(method="lm", se=FALSE, show.legend = FALSE)+
  theme_pubr(base_size = 12, margin=FALSE, border=TRUE)+
  colorblindr::scale_fill_OkabeIto(name="Lake set:")+
  colorblindr::scale_color_OkabeIto(name="Lake set:")+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Median GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma,
                breaks = my_breaks,
                limits = c(30,12000))+
  scale_y_log10(name = expression(paste('Modeled GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma,
                breaks = my_breaks,
                limits = c(30,12000))+
  # ggpp::geom_label_npc(aes(npcx = "left", npcy = "top",
  #                          label = paste("RMSE =",round(Metrics::rmse(actual_GPP_median,pred_GPP),1))),
  #                size=3, alpha=0.5)+
  ggpp::geom_label_npc(data = slopes %>% filter(name=="GPP"),
                       fill="#56B4E9",
                 aes(npcx = x, npcy = y, label = paste("SMAPE = ",
                                                       round(Metrics::smape(actual_GPP_median,pred_GPP),2),"%",
                                                       "\nRMSE = ",round(Metrics::rmse(actual_GPP_median,pred_GPP),1),"\nslope = ",estimate," (",conf.low,",",conf.high,")", sep="")),
                 size=3, alpha=0.5)

combined <- (A+B)/(C+D) & theme(legend.position="bottom",
                                legend.text.align = 0.5,
                                legend.margin = margin(0, 0, 0, 0)) 


combined +  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")


ggsave(here("figures/MS/Figure3.modeled_vs_measured_TP_DOC_zMIX_GPP_allLakes.png"), dpi=600, width=8,height=8, units="in")



```

#### Figure S7. Residuals exercise

```{r}
mod <- lm(GPPHat~medianGPP, all_compareVals)
summary(mod)
#calculate standardized residuals
standard_res <- rstandard(mod)
final_data <- cbind(all_compareVals, standard_res)
# all_compareVals <- cbind(all_compareVals, standard_res)

#Which lakes are the biggest outliers?
final_data %>%
  arrange(desc(standard_res)) %>%
  select(lakeName) %>%
  head()


final_data2 <-final_data %>% 
  select(lakeName, standard_res) %>%
  left_join(.,master_df) %>%
  select(-lakeName, -LSA_sizeclass, -HRT_sizeclass, -n, -set,
         -contains("abs"), -contains("vol"), -contains("mean"),
         -contains("mod"), -SA_km2, -SA_ha, -Qin,
         -contains("load"),
         -contains("quantile")) %>%
    mutate(across(c(medianGPP:LSA_km2), log10))  #log10 transform
    # mutate(across(c(medianGPP:LSA_km2), scale)) #then scale


# Compute a correlation matrix
corr <- round(cor(final_data2), 1)
head(corr[, 1:6])

# Compute a matrix of correlation p-values
p.mat <- cor_pmat(final_data2)
head(p.mat[, 1:4])

ggcorrplot(
  corr,
  # hc.order = TRUE,
  type = "lower",
  outline.color = "white",
  ggtheme = ggthemes::theme_few,
  colors = c("#6D9EC1", "white", "#E46726"),
  lab=TRUE
)

#OK so the biggest factors here are lake size and lake TP concentration.




#plot predictor variable vs. standardized residuals
plot(final_data2$TP_ugL, standard_res, ylab='Standardized Residuals', xlab='TP_ugL') 
#add horizontal line at 0
abline(0, 0)

#plot predictor variable vs. standardized residuals
plot(final_data2$LSA_km2, standard_res, ylab='Standardized Residuals', xlab='LSA_km2') 
#add horizontal line at 0
abline(0, 0)



all_lakes <- final_data2 %>%
  select(standard_res,LSA_km2, TP_ugL) %>%
  pivot_longer(-1) %>%
  ggplot(aes(y=standard_res, x=value))+
  geom_point(shape=21,alpha=0.5, size=2.2,fill="black") +
  geom_abline(intercept=0, slope=0)+
  facet_wrap(.~name,ncol=1,scales="free_x",labeller=my_labeller)+
  theme_pubr(base_size = 12, border=TRUE, margin=FALSE)+
  labs(y="Standardized residuals",
       title="All lakes")
all_lakes


##Do we draw the same conclusion if we just look at the 18 "calirbation" lakes?


mod <- lm(GPPHat~medianGPP, compareVals)
summary(mod)
#calculate standardized residuals
standard_res2 <- rstandard(mod)
final_data_a <- cbind(compareVals, standard_res2)
# all_compareVals <- cbind(all_compareVals, standard_res)

#Which lakes are the biggest outliers?
final_data_a %>%
  arrange(desc(standard_res2)) %>%
  select(lakeName) %>%
  head()


final_data3 <-final_data_a %>% 
  select(lakeName, standard_res2) %>%
  left_join(.,master_df) %>%
  select(-lakeName, -LSA_sizeclass, -HRT_sizeclass, -n,
         -contains("abs"), -contains("vol"), -contains("mean"),
         -contains("mod"), -SA_km2, -SA_ha, -Qin,
         -contains("load"),
         -contains("quantile")) %>%
    mutate(across(c(medianGPP:LSA_km2), log10))  #log10 transform
    # mutate(across(c(medianGPP:LSA_km2), scale)) #then scale


# Compute a correlation matrix
corr <- round(cor(final_data3), 1)
head(corr[, 1:6])

# Compute a matrix of correlation p-values
p.mat <- cor_pmat(final_data3)
head(p.mat[, 1:4])

ggcorrplot(
  corr,
  # hc.order = TRUE,
  type = "lower",
  outline.color = "white",
  ggtheme = ggthemes::theme_few,
  colors = c("#6D9EC1", "white", "#E46726"),
  lab=TRUE
)
#TP is less influential than TPin. LSA is only moderating correlated. 

subset_lakes <- final_data3 %>%
  select(standard_res2,TP_mgm3_obs, TP_ugL) %>%
  pivot_longer(-1) %>%
  ggplot(aes(y=standard_res2, x=value))+
  geom_point(shape=21,alpha=0.5, size=2.2,fill="black") +
  geom_abline(intercept=0, slope=0)+
  facet_wrap(.~name,ncol=1,scales="free_x",labeller=my_labeller)+
  theme_pubr(base_size = 12, border=TRUE, margin=FALSE)+
  labs(y=NULL,
       title="18 lake calibration set")


all_lakes + subset_lakes
ggsave(here("figures/MS/FigureS7.GPP_residuals.png"), dpi=600, width=6,height=8, units="in")




### Residuals for zMix predictions
mod <- lm(zMixHat~meanzMix, all_compareVals)
summary(mod)
#calculate standardized residuals
standard_res <- rstandard(mod)
resid <- resid(mod)
final_data <- cbind(all_compareVals, standard_res, resid)
# all_compareVals <- cbind(all_compareVals, standard_res)

#Which lakes are the biggest outliers?
final_data %>%
  arrange(desc(standard_res)) %>%
  select(lakeName) %>%
  head()


final_data2 <-final_data %>% 
  select(lakeName, standard_res) %>%
  left_join(.,master_df) %>%
  select(-lakeName, -LSA_sizeclass, -HRT_sizeclass, -n, 
         -contains("abs"), -contains("vol"), -contains("mean"),
         -contains("mod"), -SA_km2, -SA_ha, -Qin,
         -contains("load"),
         -contains("quantile")) %>%
    mutate(across(c(medianGPP:LSA_km2), log10))  #log10 transform
    # mutate(across(c(medianGPP:LSA_km2), scale)) #then scale


# Compute a correlation matrix
corr <- round(cor(final_data2), 1)
head(corr[, 1:6])

# Compute a matrix of correlation p-values
p.mat <- cor_pmat(final_data2)
head(p.mat[, 1:4])

ggcorrplot(
  corr,
  # hc.order = TRUE,
  type = "lower",
  outline.color = "white",
  ggtheme = ggthemes::theme_few,
  colors = c("#6D9EC1", "white", "#E46726"),
  lab=TRUE
)

#OK so the biggest factors here are lake size and lake TP concentration.

#plot predictor variable vs. standardized residuals
plot(final_data2$V, abs(standard_res), ylab='|zMix standardized Residuals|', xlab='log10 V') 
#add horizontal line at 0
abline(0, 0)

#plot predictor variable vs. standardized residuals
plot(final_data2$V, resid, ylab='zMix Residuals', xlab='log10 V') 
#add horizontal line at 0
abline(0, 0)

plot(final_data2$DOC_mgL,standard_res, ylab='zMix standardized Residuals', xlab='log10 DOC') 
#add horizontal line at 0
abline(0, 0)


final_data2 %>% 
  mutate(over_under=case_when(standard_res>0 ~ "over",
                              standard_res<0 ~ "under")) %>%
  ggplot(aes(x=V, y=standard_res, fill=over_under))+
  geom_point(shape=21)+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1)


final_data2 %>% 
  ggplot(aes(x=V, y=standard_res, fill=log10(DOC_mgL)))+
  geom_point(shape=21)+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1)

```

### Figure 6. Parameter comparisons

Create the skeleton for an eventual figure that shows the updated parameters and ranges

```{r}
lit_params <- read.csv("data/metropolis_results/ParameterEstimatesLiterature.csv") %>%
  select(-description, -sources) %>%
  mutate(set="literature") %>% #call this kelly for now so we have the kelly values map onto the literature range
         # value=(min+max)/2) %>%
  rename(low=min,
         high=max)
  # pivot_longer(2:3)


gridSearchQuantiles_wide<-read.csv(here("data/metropolis_results/finalRun_Quantiles_2023-12-06.csv"),
                 stringsAsFactors = FALSE) %>%
  pivot_longer(pA:rec) %>%
  mutate(value=exp(value)) %>% #Need to exponentiate for correct units
  pivot_wider(names_from = quantile) %>%
  mutate(set = "this study") %>%
  rename(parameter=name,
         low=q_0.025,
         high=q_0.975,
         # low=q_0.025,
         # high=q_0.975,
         value=q_0.5)

kelly<-read.csv(here("data/preliminaryMetropolisResults/kellymodel_parametersets4bella.csv"),
                 stringsAsFactors = FALSE) %>%
  rename(set=X) %>%
  filter(set=="kelly") %>%
  pivot_longer(pA:rec, names_to="parameter", values_to="value") 




# Parameter comparison plots (lit range, kelly, and best optim
combine<-bind_rows(lit_params,
          gridSearchQuantiles_wide,
          kelly) %>%
  mutate(set=factor(set,
                    levels=c("kelly",
                             "literature",
                             "this study"),
                    labels=c("original model",
                             "literature range",
                             "this study"))) 

my_labeller <- as_labeller(c(cA="c[A]",
                             decay="d",
                             hA="h[A]",
                             kA="k[A]",
                             kDOC="k[DOC]",
                             lA="l[A]",
                             mA="m[A]",
                             pA="p[A]",
                             rec="q",
                             v="v"),
                           default = label_parsed)

combine%>%
  ggplot() + 
  geom_errorbar(aes(x=set, value=value, group=set, color=set, ymin=low, ymax=high), width=.5,
                 position=position_dodge(.9)) +
  geom_point(aes(x=set, y=value, group=set, fill=set, color=set), color="black", shape=21)+
  colorblindr::scale_fill_OkabeIto(order = c(1,2,6),
                                   name="Legend:")+
  colorblindr::scale_color_OkabeIto(order = c(1,2,6),
                                    name="Legend:")+
  # scale_color_manual(values=c("black","purple","blue","orange"))+
  facet_wrap(~parameter, scales="free", ncol=5, labeller=my_labeller) +
  theme_few()+
  theme(legend.position="bottom",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        legend.margin = margin(0, 0, 0, 0))+
  labs(y="Parameter value")
ggsave(here("figures/MS/Figure6.point_plots_kelly_optim_lit_parameter_ranges.png"), dpi=600, width=7,height=3.5, units="in")



```

### Table 1 - lake metadata

```{r}

#morphometry and nutrient dataframe
morphNut<-master_df%>%
  select(lakeName, SA_ha, HRT_days, V, zMean, DOC_mgL, TP_ugL) %>%
  mutate(volume_1000000m3=V/1000000,
         HRT_years=HRT_days/365)%>%
  select(-V, -HRT_days) %>%
  left_join(.,metadata %>%
                select(lakeName, `Latitude (decimal degrees)`, `Longitude (decimal degrees)`)) %>%
  mutate(across(where(is.numeric), round, 2)) #round all numeric values to 2 decimal places

#metab dataframe
metab<-bella_metab_withMetaData_extendedSummer %>%
  group_by(lakeName)%>%
  skim()%>%
  filter(skim_type=="numeric") %>%
  filter(skim_variable %in% c("GPP_mgCm2", "zMix")) %>%
  select(skim_variable, lakeName, numeric.p50, numeric.sd) %>%
  # mutate(
  #   numeric.mean=round(numeric.mean,0),
  #   numeric.sd=round(numeric.sd,1),
  #   meansd = paste0(numeric.mean, " (",numeric.sd,")")) %>%
  # select(-numeric.mean,-numeric.sd) %>%
    mutate(
    numeric.p50=round(numeric.p50,0)) %>%
  select(-numeric.sd) %>%
  pivot_wider(names_from = "skim_variable", values_from = "numeric.p50") %>%
  filter(lakeName %in% morphNut$lakeName)  

###EXPORT TABLE
summary_table_export <- 
  right_join(morphNut, metab, by="lakeName")%>%
  hux() %>% 
  arrange(lakeName) %>%
  relocate(`Latitude (decimal degrees)`, .after = lakeName) %>% #rearranging in case this presents a problem in modeling
  relocate(`Longitude (decimal degrees)`, .after = `Latitude (decimal degrees)`)%>%
  rename(`Lake name`=lakeName,
         `SA (ha)`=SA_ha,
         `HRT (years)`=HRT_years,
         `V (1000000 m3)`=volume_1000000m3,
         `Mean depth (m)`=zMean,
         `DOC (g m-3)`=DOC_mgL,
         `TP (mg m-3)`= TP_ugL,
         # `DOC load (kg)`=DOC_load_kg,
         # `TP load (kg)`=TP_load_kg,
         # `Inflow C:P`=CP,
         `GPP (mg C m2 day)`=GPP_mgCm2,
         `Mixed layer depth (m)`=zMix)%>%
  add_colnames() %>%
  add_footnote("SA = lake surface area; HRT = hydrologic residence time; V = lake volume; 
               DOC = mixed layer dissolved organic carbon concentration; TP = mixed layer total phosphorus concentration;
               GPP = gross primary production") %>%
  set_bold(row = 1, col = everywhere, value = TRUE) %>% 
  set_all_borders(TRUE) %>%
  set_all_padding(0) %>%
  set_outer_padding(0) %>%
  theme_article()
# 
# summary_table_export<-summary_table_export %>%
#   set_all_padding(0) %>%
#   set_outer_padding(0) 
library(officer)
library(flextable)
huxtable::quick_docx(summary_table_export, file = 'Figures/MS/TableS1.docx')




ft <- as_flextable(summary_table_export) 

summary_table_export<-regulartable(summary_table_export) %>% 
  padding(padding = 0, part = "all") %>% 
  autofit() %>% 
  # dim_pretty() %>%
  # width(c(0.6797418,  1.8088650 , 1.9363539,  0.6797418,  0.7808974 , 1.0196126 ,
  #         1.0783963 , 0.8233439 , 0.6367730 , 1.1470269 , 0.9770915 , 0.6962280 , 1.3752984 ,
  #         1.4773492)) %>%
  print(preview = "docx")

# sect_properties <- prop_section(
#   page_size = page_size(orient = "landscape",
#     width = 8.3, height = 11.7),
#   type = "continuous",
#   page_margins = page_mar(),
#   section_columns = section_columns(widths = c(4.75, 4.75))
# )
save_as_docx(summary_table_export, pr_section = sect_properties,
             path = 'results/draft MS figs/tables/Table1_testing.docx')


```


##### SIL nutrient & load figs

```{r}


  my_labeller <- as_labeller(c(
                             DOC_mgL="log[10]~DOC~(mg~L^1)", TP_ugL="log[10]~TP~(mu~g~L^-1)"),
                             
                           default = label_parsed)

A<-master_df %>%
  select(lakeName,DOC_mgL,TP_ugL) %>%
  pivot_longer(-1) %>%
  ggplot()+
  geom_density(aes(x = log10(value)),fill="#5E813F") +
  geom_rug(aes(x = log10(value), y = 0), sides="b") +
  facet_wrap(.~name, scales="free",ncol=2,
             labeller=my_labeller, strip.position="bottom")+
  theme_pubr(base_size = 16)+
  theme(legend.position="none",
        axis.title.x=element_blank(),
        strip.placement = "outside",
        strip.background = element_blank(),
        # strip.text = element_text(size=12),
        # axis.text.y= element_text(size=12),
        )
ggsave(here("figures/SIL/TP_DOC_distributions.png"), width=7, height=1.5,units="in", dpi=600)



B<-master_df %>%
  ggplot(aes(y=DOC_mgL, x=TP_ugL,
             # fill=DOC_mgL/(TP_ugL),
             ))+
  geom_point(shape=21, size=3, color="black", fill="grey50", alpha=0.5)+
  geom_abline(intercept = 0, slope = 1, linetype="dashed")+
  xlab(expression('Mean lake TP ug L'^-1))+
  ylab(expression(paste('Mean lake DOC mg L'^-1,)))+
  # geom_text_repel(aes(label = lakeName),box.padding = 0.5, max.overlaps = 2) +
  theme_pubr()+
    theme(legend.position = c(0.02, 0.89),
        legend.justification = c("left", "bottom"),
        legend.box.just = "right",
        legend.title.align = 0,
        legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.background = element_blank(),
        legend.margin = margin(0, 0, 0, 0),
        legend.title = element_text(size=10))
ggsave(here("figures/SIL/TPvsDOC_label_outliers.png"), width=7, height=5,units="in", dpi=600)


(A/B)+plot_layout(heights = c(0.2,1))
ggsave(here("figures/SIL/TPvsDOC_density.png"), width=9, height=7,units="in", dpi=600)


master_df %>%
  filter(!dataset=="NA")%>%
  ggplot(aes(y=DOC_load_kg, x=TP_load_kg, fill=log10(SA_ha)))+
  geom_point(shape=21, size=3, color="black",  alpha=0.5)+
  geom_abline(intercept = 0, slope = 1, linetype="dashed")+
  xlab(expression('Mean TP load kg day'^-1))+
  ylab(expression(paste('Mean DOC load kg day'^-1,)))+
  scale_x_log10(labels = scales::comma)+
  scale_y_log10(labels = scales::comma)+
  theme_pubr()+
  scale_fill_gradient(name="log10 surface area (ha)",
                      low = "darkblue", high = "red")+
  # geom_text_repel(aes(label = lakeName),box.padding = 0.5, max.overlaps = 2) +
  #Moves legend to inside of plot
  theme(legend.position = c(0.05, 0.65),
        legend.justification = c("left", "bottom"),
        legend.box.just = "right",
        legend.title.align = 0,
        legend.direction = "vertical",
        legend.box = "vertical",
        legend.background = element_blank(),
        legend.margin = margin(0, 0, 0, 0),
        legend.title = element_text(size=10))
ggsave(here("figures/SIL/TPvsDOC_loads.png"), width=7, height=5,units="in", dpi=600)


```

#### Supp Fig- transformations C:P

```{r}
#Paired points -- how does Stream C:P compare to Lake C:P?

#DOC
pairedDF1<- master_df %>%
  filter(!lakeName=="LittleRock")%>%
  select(lakeName, DOC_gm3, DOC_mgL) %>%
  drop_na() %>%
  mutate(cat="DOC")
  
ggpaired(pairedDF1, cond1 = "DOC_gm3", cond2 = "DOC_mgL", line.color = "gray", line.size = 0.4,
         fill = "condition", palette = "jco")


#TP
pairedDF2<- master_df %>%
  filter(!lakeName=="LittleRock")%>%
  select(lakeName,  TP_mgm3, TP_ugL) %>%
  drop_na() %>%
  mutate(cat="TP")
  
ggpaired(pairedDF2, cond1 = "TP_mgm3", cond2 = "TP_ugL", line.color = "gray", line.size = 0.4,
         fill = "condition", palette = "jco")

#Stoich
pairedDF<- master_df %>%
  filter(!lakeName=="LittleRock")%>%
  mutate(loadCP=(DOC_gm3/(TP_mgm3)),
         lakeCP=(DOC_mgL/TP_ugL))  %>%
  select(lakeName,dataset, loadCP, lakeCP) 

ggpaired(pairedDF, cond1 = "loadCP", cond2 = "lakeCP", line.color = "gray50", line.size = 0.4,
         fill = "condition", palette = "jco")


#logStoch
pairedDF3<- master_df %>%
  filter(!lakeName=="LittleRock")%>%
  mutate(loadCP=log(DOC_gm3/(TP_mgm3)),
         lakeCP=log(DOC_mgL/TP_ugL))  %>%
  select(lakeName, loadCP, lakeCP) %>%
  mutate(cat="stoich")

ggpaired(pairedDF3, cond1 = "loadCP", cond2 = "lakeCP", line.color = "gray50", line.size = 0.4,
         fill = "condition", palette = "jco")


#All params
pairedDF1<- pairedDF1 %>%
  rename(inflow=DOC_gm3,
         lake=DOC_mgL)
pairedDF2<- pairedDF2 %>%
  rename(inflow=TP_mgm3,
         lake=TP_ugL)
pairedDF3<- pairedDF3 %>%
  rename(inflow=loadCP,
         lake=lakeCP)
allPaired<-bind_rows(pairedDF1,pairedDF2,pairedDF3) %>%
  mutate(cat=factor(cat,
                    levels=c("DOC","TP","stoich")))



ggpaired(allPaired, cond1 = "inflow", cond2 = "lake", line.color = "gray50", line.size = 0.4,
         fill = "condition", palette = "jco") +
  facet_wrap(.~cat, scales="free_y")

# ggsave(here("results/draft MS figs/SuppFig.PairedPoints_C_P_CP.png"), width=9, height=8,units="in", dpi=600)


#Is there even a relationship between load C:P and lake C:P? 
cor.test(pairedDF3$inflow, pairedDF3$lake)

#Ugh, but across lakes, load and lake C:P are highly correlated. 
pairedDF3 %>%
  ggplot(aes(x=lake, y=inflow))+
  geom_point(shape=21,size=3, fill="grey50")

#Unless you don't log transform, but you should. 
master_df %>%
    mutate(loadCP=(DOC_gm3/(TP_mgm3)),
         lakeCP=(DOC_mgL/TP_ugL))  %>% 
    ggplot(aes(x=lakeCP, y=loadCP))+
  geom_point(shape=21,size=3, fill="grey50")
```

#### Linear models

```{r}
GPP_linearmodels<-
#Modeled GPP
bind_rows(master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
  left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  select(PPareal, DOC_load_kgday_mean, TP_load_kgday_mean,
         meanzMix, TP_ugL, DOC_mgL, LSA_km2, V_m3) %>%
  mutate(CPload=DOC_load_kgday_mean/(TP_load_kgday_mean*1000),
         CPlake=DOC_mgL/TP_ugL) %>%
  pivot_longer(-PPareal) %>%
  rename(predictor=name) %>%
  nest(data = -c(predictor)) %>%
  mutate(
    fit = map(data, ~lm(log10(PPareal) ~ log10(value), data = .x)),
    tidy_out = map(fit, tidy)
  ) %>%
  unnest(cols = tidy_out) %>%
  select(-fit, -data) %>%
  filter(term != "(Intercept)") %>%
  # filter(p.value<0.05) %>%
  mutate(dataset="modeled GPP"),

#Observed GPP
master_df %>%
  select(meanGPP, DOC_load_kgday_mean, TP_load_kgday_mean,
         meanzMix, TP_ugL, DOC_mgL, LSA_km2, V_m3) %>%
  mutate(CPload=DOC_load_kgday_mean/(TP_load_kgday_mean*1000),
         CPlake=DOC_mgL/TP_ugL) %>%
  pivot_longer(-meanGPP) %>%
  rename(predictor=name) %>%
  nest(data = -c(predictor)) %>%
  mutate(
    fit = map(data, ~lm(log10(meanGPP) ~ log10(value), data = .x)),
    tidy_out = map(fit, tidy)
  ) %>%
  unnest(cols = tidy_out) %>%
  select(-fit, -data) %>%
  filter(term != "(Intercept)") %>%
  # filter(p.value<0.05) %>%
  mutate(dataset="measured GPP"))

GPP_linearmodels


```

#### \~SIL figures

```{r}


####Both on the same graph
#### Also try this with the PREDICTED GPP values. 
#Do we qualitatively see the patterns that we would expect? 
#Why can't I recreate the data from before? 
my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
                             meanzMix="log[10]~Mixed~layer~depth~(m)", V_m3="log[10]~Volume~(m^3)",
                             DOC_mgL="DOC~(mg~L^1)", TP_ugL="TP~(mu~g~L^-1)",
                             DOC_load_kgday_mean="Mean~daily~DOC~load~(kg)",
                             TP_load_kgday_mean="Mean~daily~TP~load~(kg)",
                             CP="DOC~to~TP~ratio~(loads)"),
                             
                           default = label_parsed)


  plot_data<-  master_df %>%
    left_join(., fits_kelly_long,  by="lakeName") %>%
    pivot_wider(names_from=name,values_from=value) %>%
    left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  select(lakeName, meanGPP, GPPHat, DOC_load_kgday_mean, TP_load_kgday_mean,
         meanzMix, TP_ugL, DOC_mgL, LSA_km2, V_m3) %>%
  mutate(CP=DOC_load_kgday_mean/(TP_load_kgday_mean*1000)) %>%
  pivot_longer(-(1:3)) %>%
  rename(predictor_variable=name,
         predictor_value=value)%>%
  mutate(predictor_variable=factor(predictor_variable,
                     levels=c("DOC_load_kgday_mean","DOC_mgL",
                              "TP_load_kgday_mean","TP_ugL",
                              "CP","meanzMix",
                              "LSA_km2","V_m3")))%>%
  pivot_longer(meanGPP:GPPHat) %>%
  rename(response_variable=name,
         response_value=value)%>%
  filter(response_value>1)%>%
  mutate(response_variable=factor(response_variable,
                     labels=c("Measured GPP","Modeled GPP")))

predGPP_correlations<- plot_data %>%
  # filter(response_variable=="Modeled GPP")%>%
  ggplot(aes(x=predictor_value, y=response_value, fill=response_variable, linetype=response_variable))+
  geom_point(size=3, shape=21, color="black", alpha=0.5)+
  geom_smooth(method="lm", se=T, color="black")+
  facet_wrap(.~predictor_variable, scales="free",ncol=2,
             labeller=my_labeller, strip.position="bottom")+
  scale_y_log10(expression(paste('Predicted GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma, breaks_log(n = 6, base = 10))+
  scale_fill_manual(values=c("#ffc300","#003566"),
                    name="Legend:")+
  scale_linetype_manual(values=c(1,6),
                    name="Legend:")+
  scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
  theme_classic()+
  theme(axis.title.x=element_blank(),
        strip.placement = "outside",
        strip.background = element_blank(),
        plot.margin = margin(1,1,0.5,0.5, "cm") 
        # strip.text = element_text(size=12),
        # axis.text.y= element_text(size=12),
        )

predGPP_correlations  
ggsave(here("figures/SIL/correlationsWithGPP.png"), width=9, height=8,units="in", dpi=600)



#Just measured values
  my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
                             meanzMix="log[10]~Mixed~layer~depth~(m)", V_m3="log[10]~Volume~(m^3)",
                             DOC_mgL="DOC~(mg~L^1)", TP_ugL="TP~(mu~g~L^-1)",
                             DOC_load_kgday_mean="Mean~daily~DOC~load~(kg)",
                             TP_load_kgday_mean="Mean~daily~TP~load~(kg)",
                             CP="DOC~to~TP~ratio~(loads)"),
                             
                           default = label_parsed)

predGPP_correlations<- plot_data %>%
  filter(response_variable=="Measured GPP")%>%
  ggplot(aes(x=predictor_value, y=response_value, fill=response_variable, linetype=response_variable))+
  geom_point(size=3, shape=21, color="black", alpha=0.5)+
  geom_smooth(method="lm", se=T, color="black")+
  facet_wrap(.~predictor_variable, scales="free",ncol=2,
             labeller=my_labeller, strip.position="bottom")+
  scale_y_log10(expression(paste('Measured or predicted GPP (mg C m'^-2,' day'^-1,')')),
                labels = scales::comma, breaks_log(n = 6, base = 10))+
  scale_fill_manual(values=c("#ffc300","#003566"),
                    name="Legend:")+
  scale_linetype_manual(values=c(1,6),
                    name="Legend:")+
  scale_x_log10("SA (ha)",labels = scales::comma, breaks_log(n = 5, base = 10))+
  theme_classic()+
  theme(axis.title.x=element_blank(),
        strip.placement = "outside",
        strip.background = element_blank(),
        plot.margin = margin(1,1,0.5,0.5, "cm") 
        # strip.text = element_text(size=12),
        # axis.text.y= element_text(size=12),
        )

predGPP_correlations  

ggsave(here("figures/SIL/correlationsWithGPP_measured_only.png"), width=9, height=8,units="in", dpi=600)

```

#### PCA?

```{r}


pca_data<-master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
  left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  filter(!lakeName %in% c("Kentucky"))%>% #extreme outlier with surface area, so dropping this for the sake of the analysis right now. 
  select(lakeName, DOC_load_kgday_mean, TP_load_kgday_mean,
         meanzMix, TP_ugL, DOC_mgL, LSA_km2, V_m3) %>%
    mutate(CP=DOC_load_kgday_mean/(TP_load_kgday_mean*1000)) %>%
  drop_na()

pca_data_withPPareal<-master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
  left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  filter(!lakeName %in% c("Kentucky"))%>% #extreme outlier with surface area, so dropping this for the sake of the analysis right now. 
  select(lakeName, PPareal,DOC_load_kgday_mean, TP_load_kgday_mean,
         meanzMix, TP_ugL, DOC_mgL, LSA_km2, V_m3) %>%
    mutate(CP=DOC_load_kgday_mean/(TP_load_kgday_mean*1000)) %>%
  drop_na()


pca_fit<-pca_data %>%
  drop_na()%>%
  select(where(is.numeric)) %>% # retain only numeric columns
  scale() %>%                   # scale to zero mean and unit variance
  prcomp()                      # do PCA


pca_fit %>%
  # add PCs to the original dataset
  augment(pca_data_withPPareal) %>%
  ggplot(aes(.fittedPC1, .fittedPC2)) +
  geom_point(size=3, shape=21, aes(fill = log10(PPareal)))+
  geom_text_repel(aes(label = lakeName), size = 3)+
    scale_fill_continuous_sequential(palette = "Terrain", rev=TRUE)


pca_fit %>%
  # add PCs to the original dataset
  augment(pca_data_withPPareal) %>%
  ggplot(aes(PPareal, .fittedPC1)) +
  geom_point(size=3, shape=21, aes(fill = log10(PPareal)))+
  geom_text_repel(aes(label = lakeName), size = 3)+
  scale_fill_continuous_sequential(palette = "Terrain", rev=TRUE)

pca_fit %>%
  # add PCs to the original dataset
  augment(pca_data_withPPareal) %>%
  ggplot(aes(PPareal, .fittedPC2)) +
  geom_point(size=3, shape=21, aes(fill = log10(PPareal)))+
  geom_text_repel(aes(label = lakeName), size = 3)+
  scale_fill_continuous_sequential(palette = "Terrain", rev=TRUE)

pca_fit %>%
  # add PCs to the original dataset
  augment(pca_data_withPPareal) %>%
  ggplot(aes(PPareal, .fittedPC3)) +
  geom_point(size=3, shape=21, aes(fill = log10(PPareal)))+
  geom_text_repel(aes(label = lakeName), size = 3)+
  scale_fill_continuous_sequential(palette = "Terrain", rev=TRUE)


#########################
#Plot the rotation matrix
#########################
arrow_style <- arrow(
  angle = 20, length = grid::unit(8, "pt"),
  ends = "first", type = "closed"
)
pca_fit %>%
  tidy(matrix = "rotation") %>%
  pivot_wider(
    names_from = "PC", values_from = "value",
    names_prefix = "PC"
  ) %>%

  ggplot(aes(PC1, PC2)) +
  geom_segment(
    xend = 0, yend = 0,
    arrow = arrow_style
  ) +
  geom_text(aes(label = column), hjust = 1) +
  xlim(-0.6,0.6) + ylim(-0.5,0.7) +
  coord_fixed()


#########################
#Plot the variance explained
#########################

pca_fit %>%
  # extract eigenvalues
  tidy(matrix = "eigenvalues") %>%
  ggplot(aes(PC, percent)) + 
  geom_col() + 
  scale_x_continuous(
    # create one axis tick per PC
    breaks = 1:6
  ) +
  scale_y_continuous(
    name = "variance explained",
    # format y axis ticks as percent values
    label = scales::label_percent(accuracy = 1)
  )
```

### Fig4- GPP versus DOC

```{r}
MeanGPPvDOCvloadCP<-master_df %>%
  filter(!lakeName=="Annie")%>%
  mutate(CP=DOC_load_kgday_mean/(TP_load_kgday_mean)) %>%
  ggplot(aes(y=medianGPP, x=DOC_mgL, size=LSA_sizeclass, fill=CP))+
  geom_point(shape=21, color="black")+
  scale_fill_continuous_sequential(palette = "Lajolla", rev=FALSE,
                                   name="Load DOC:TP\n(mass:mass)")+
    scale_size_discrete(name=expression(paste('Lake size (km'^2,')')))+
  theme_pubr(base_size = 18)+
  ylab(expression(paste('Median GPP (mg C m'^-2,' day'^-1,')')))+
  xlab(expression(paste('DOC (mg L'^-1,')')))+
      theme(legend.position = c(.90, 0.9),
        legend.justification = c("right", "top"),
        legend.box.just = "right",
        legend.title.align = 0,
        legend.direction = "horizontal",
        legend.box = "vertical",
        legend.background = element_blank(),
        legend.margin = margin(0, 0, 0, 0))+
    guides(size = guide_legend(label.position = "bottom", 
            title.position = "left", title.vjust = 0.8))

MeanGPPvDOCvlakeCP<-
  master_df %>%
  filter(!lakeName=="Annie")%>%
  ggplot(aes(y=medianGPP, x=DOC_mgL, size=LSA_sizeclass, fill=DOC_mgL/TP_ugL))+
  geom_point(shape=21, color="black")+
  scale_fill_continuous_sequential(palette = "Lajolla", rev=FALSE,
                                   name="Lake DOC:TP\n(mass:mass)")+
  scale_size_discrete(name=expression(paste('Lake size (km'^2,')')))+
  theme_pubr(base_size = 18)+
  ylab(expression(paste('Median GPP (mg C m'^-2,' day'^-1,')')))+
  xlab(expression(paste('DOC (mg L'^-1,')')))+
      theme(legend.position = c(.90, 0.9),
        legend.justification = c("right", "top"),
        legend.box.just = "right",
        legend.title.align = 0,
        legend.direction = "horizontal",
        legend.box = "vertical",
        legend.background = element_blank(),
        legend.margin = margin(0, 0, 0, 0))+
    guides(size = guide_legend(label.position = "bottom", 
            title.position = "left", title.vjust = 0.8))

# MedianGPPvDOCvloadCP<-master_df %>%
#   filter(!lakeName=="Annie")%>%
#   mutate(CP=DOC_gm3/(TP_gm3*1000)) %>%
#   ggplot(aes(y=medianGPP, x=DOC_mgL, size=LSA_sizeclass, fill=CP))+
#   geom_point(shape=21, color="black")+
#   scale_fill_continuous_sequential(palette = "Lajolla", rev=FALSE,
#                                    name="Load DOC:TP\n(mass:mass)")+
#     scale_size_discrete(name=expression(paste('Lake size (km'^2,')')))+
#   theme_classic()+
#   ylab(expression(paste('Median GPP (mg C m'^-2,' day'^-1,')')))+
#   xlab(expression(paste('DOC (mg L'^-1,')')))+
#       theme(legend.position = c(.90, 0.9),
#         legend.justification = c("right", "top"),
#         legend.box.just = "right",
#         legend.title.align = 0,
#         legend.direction = "horizontal",
#         legend.box = "vertical",
#         legend.background = element_blank(),
#         legend.margin = margin(0, 0, 0, 0),
#         legend.title = element_text(size=10))+
#     guides(size = guide_legend(label.position = "bottom", 
#             title.position = "left", title.vjust = 0.8))
# 
# MedianGPPvDOCvlakeCP<-
#   master_df %>%
#   filter(!lakeName=="Annie")%>%
#   mutate(CP=DOC_gm3/(TP_gm3*1000)) %>%
#   ggplot(aes(y=medianGPP, x=DOC_mgL, size=LSA_sizeclass, fill=DOC_mgL/TP_ugL))+
#   geom_point(shape=21, color="black")+
#   scale_fill_continuous_sequential(palette = "Lajolla", rev=FALSE,
#                                    name="Lake DOC:TP\n(mass:mass)")+
#   scale_size_discrete(name=expression(paste('Lake size (km'^2,')')))+
#   theme_classic()+
#   ylab(expression(paste('Median GPP (mg C m'^-2,' day'^-1,')')))+
#   xlab(expression(paste('DOC (mg L'^-1,')')))+
#       theme(legend.position = c(.90, 0.9),
#         legend.justification = c("right", "top"),
#         legend.box.just = "right",
#         legend.title.align = 0,
#         legend.direction = "horizontal",
#         legend.box = "vertical",
#         legend.background = element_blank(),
#         legend.margin = margin(0, 0, 0, 0),
#         legend.title = element_text(size=10))+
#     guides(size = guide_legend(label.position = "bottom", 
#             title.position = "left", title.vjust = 0.8))



ggarrange(MeanGPPvDOCvloadCP,MeanGPPvDOCvlakeCP,
          # MedianGPPvDOCvloadCP,MedianGPPvDOCvlakeCP,
          # nrow=2,
          ncol=2,
                     labels = c("A", "B"),
                     font.label = list(size = 12, face="bold"))

# ggsave(here("results/draft MS figs/GPPversusDOC_FnOfCPandSA.png"), width=12, height=6,units="in", dpi=600)
ggsave(here("figures/SIL/GPPversusDOC_FnOfCPandSA.png"), width=12, height=6,units="in", dpi=600)



MeanGPPvDOCvloadCP
ggsave(here("figures/GLEON/GPPversusDOC_FnOfCPandSA.png"), width=12, height=6,units="in", dpi=600)

```


### SUPP FIGS

#### Figure S1. Maps

```{r}


metadata$inflows_YN <- factor(metadata$inflows_YN, levels = c("yes", "no"), 
                  labels = c("Lakes with surface inflows", "Lakes without surface inflows"))

length(unique(metadata$Country))


world <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")


# generate world map
GLEON_global_map_alt<-ggplot(data = world) +
  geom_sf() +
  # coord_sf(xlim= c(-120,NA), ylim = c(-50, NA)) +
  labs( x = "Longitude", y = "Latitude") +
  # ggtitle("Global distribution of lakes for Kelly model test ")+
  geom_jitter(data=master_df %>%
              left_join(., metadata %>%
                    select(lakeName, inflows_YN,`Longitude (decimal degrees)`,
                           `Latitude (decimal degrees)`), by="lakeName") ,
             aes(x=`Longitude (decimal degrees)`,
                 y=`Latitude (decimal degrees)`,
                 fill=inflows_YN),
             shape=21, color="black", size=2, fill="#ffba08")+
    # scale_fill_manual(values=c("#52b788","#FFFFFF"))+
  theme_pubr(base_size=18)+
        theme(
        axis.line = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        plot.background = element_rect(fill="#98dbfa"),
        panel.background = element_rect(fill="#98dbfa")
      )
GLEON_global_map_alt
  # ggsave(here("figures/SIL/global_map.png"), width=10, height=9,units="in", dpi=600)


#Morphometry density plots or histograms


#density plot
my_labeller <- as_labeller(c(HRT_days="log[10]~HRT~(days)", LSA_km2="log[10]~Surface~Area~(km^2)",
                             zMean="log[10]~Mean~depth~(m)", V_m3="log[10]~Volume~(m^3)"),
                           default = label_parsed)

density_plot<-master_df %>%
  select(HRT_days, LSA_km2, zMean, V_m3) %>%
  pivot_longer(1:4)%>%
  # mutate(name= factor(name, labels=c(expression('hi'[5]*'there'[6]^8*'you'[2]),
  #                                    "beta",
  #                                    "gamma",
  #                                    "poop")))%>%
  ggplot()+
  geom_density(aes(x = log10(value), fill=name)) +
  geom_rug(aes(x = log10(value), y = 0), sides="b")+
  facet_wrap(~name, nrow=2, scales="free",
             labeller=my_labeller, strip.position="bottom")+
  scale_fill_manual(values=c("#5f0f40","#9a031e","#fb8b24","#e36414"))+
  # theme_minimal()+
  theme_pubr(base_size = 18)+
  theme(legend.position="none",
        axis.title.x=element_blank(),
        strip.placement = "outside",
        strip.background = element_blank(),
        # strip.text = element_text(size=12),
        # axis.text.y= element_text(size=12),
        )+
  ylab("Density")
 density_plot
 
 ##ASSEMBLE
 top<-(GLEON_global_map_alt | density_plot )
 Fig1<-((GLEON_global_map_alt | density_plot ) / GPP_distribution ) +
   plot_layout(heights = c(1, 1.8))+
   plot_annotation(tag_levels = 'A')
  Fig1
  
  ggsave(here("results/draft MS figs/studyMapAndDistributions.png"), width=10, height=9,units="in", dpi=600)

```

##### SIL maps & violin plots

```{r}


world <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")
library(spData)
library(sf)
data("us_states", package = "spData")
# us_states_2163 = st_transform(us_states, crs = 2163)
us_states_bb = sf::st_as_sfc(sf::st_bbox(us_states))

data("world", package = "spData")
europe <- world %>% filter(continent %in% c("Europe")) %>% filter(name_long %in% metadata$Country)
europe_bb = st_as_sfc(st_bbox(europe))
NZ <- world %>% filter(name_long=="New Zealand")
NZ_bb = st_as_sfc(st_bbox(NZ))
asia <-  world %>% filter(continent %in% c("Asia"))
asia_subset <- world %>% filter(name_long %in% c("China","Taiwan"))
asia_subset_bb = st_as_sfc(st_bbox(asia_subset))

world_map = map_data("world") %>% 
  filter(! long > 180) 

#Country labels
region.lab.data <- world_map %>%
  filter(region %in% metadata$Country) %>%
  group_by(region) %>%
  summarise(long = mean(long), lat = mean(lat)) %>% #This works for small countries but not large ones
  mutate(long = replace(long, region=="United States", -100),
         lat = replace(lat, region=="United States", 37))

GLEON_global_map_alt <- world_map %>%
  ggplot() +
  geom_map(
    map = world_map,
    aes(long, lat, map_id = region),
    color = "white", fill = "grey50", linewidth = 0.1
  ) +
  expand_limits(x = world_map$long, y = world_map$lat) +
  geom_point(data=master_df %>%
              left_join(., metadata %>%
                    select(lakeName, `Load calc possible`,`Longitude (decimal degrees)`,
                           `Latitude (decimal degrees)`), by="lakeName") %>%
               rename(load_calc=`Load calc possible`),
             aes(x=`Longitude (decimal degrees)`,
                 y=`Latitude (decimal degrees)`,
                 fill=load_calc),
             shape=21, color="black", size=2)+
  scale_fill_manual(values=c("#56B4E9","#E69F00"))+
  coord_map("moll", clip="on") +
  theme_map()+
  theme(legend.position="none")
GLEON_global_map_alt

ggsave(here("figures/MS/FigureS1.GlobalMap.png"), width=8, height=4,units="in", dpi=600)



# north_america <- ne_countries(continent = "north america",scale = "medium", returnclass = "sf")
north_america <- world_map %>% filter(region %in% c("USA", "Canada")) %>% filter(!subregion %in% c("Hawaii",
                                                                                                   "Alaska"))

GLEON_NA <- north_america %>%
  ggplot() +
  geom_map(
    map = north_america,
    aes(long, lat, map_id = region),
    color = "white", fill = "grey50", linewidth = 0.1
  ) +
  # expand_limits(x = north_america$long, y = north_america$lat) +
  geom_point(data=master_df %>%
              left_join(., metadata %>%
                    select(lakeName, `Load calc possible`,`Country`,`Longitude (decimal degrees)`,
                           `Latitude (decimal degrees)`), by="lakeName") %>%
               rename(load_calc=`Load calc possible`) %>%
                filter(Country %in% c("USA","Canada")),
             aes(x=`Longitude (decimal degrees)`,
                 y=`Latitude (decimal degrees)`,
                 fill=load_calc),
             shape=21, color="black", size=4, alpha=0.8)+
  scale_fill_manual(values=c("#56B4E9","#E69F00"))+
  coord_map("moll",xlim= c(-120,-60), ylim = c(25, 50)) +
  labs(x="Longitude",
       y="Latitude") + 
  theme(panel.grid.major=element_line("grey80"),
        plot.background = element_rect(color="black", size=2),
        panel.border = element_rect(color="grey80", fill=NA),
        panel.background = element_rect(fill="white"),
        legend.position = "none")
GLEON_NA
ggsave(here("figures/MS/FigureS1.NorthAmerica.png"), width=4.6, height=2.5,units="in", dpi=600)



  
# europe <- ne_countries(continent = "europe",scale = "medium", returnclass = "sf")
europe <- world_map %>% filter(long>-10 & long<30) %>% filter(lat>50&lat<70)

GLEON_EURO <- europe %>%
  ggplot() +
  geom_map(
    map = europe,
    aes(long, lat, map_id = region),
    color = "white", fill = "grey50", linewidth = 0.1
  ) +
  geom_point(data=master_df %>%
              left_join(., metadata %>%
                     select(lakeName, `Load calc possible`,`Country`,`Longitude (decimal degrees)`,
                           `Latitude (decimal degrees)`), by="lakeName") %>%
               rename(load_calc=`Load calc possible`) %>%
              filter(!Country %in% c("USA","Canada","New Zealand","Taiwan","China")),
             aes(x=`Longitude (decimal degrees)`,
                 y=`Latitude (decimal degrees)`,
                 fill=load_calc),
             shape=21, color="black", size=4, alpha=0.8)+
  scale_fill_manual(values=c("#56B4E9","#E69F00"))+
  coord_map("moll", xlim= c(-10,25), ylim = c(50, 70)) +
  labs(x="Longitude",
       y="Latitude") + 
  theme(panel.grid.major=element_line("grey80"),
        plot.background = element_rect(color="black", size=2),
        panel.border = element_rect(color="grey80", fill=NA),
        panel.background = element_rect(fill="white"),
        legend.position = "none")
GLEON_EURO
ggsave(here("figures/MS/FigureS1.Europe.png"), width=3.5, height=2.5,units="in", dpi=600)


NZ <- world_map %>% filter(region == "New Zealand" & subregion %in% c("North Island","South Island"))

GLEON_NZ <- NZ %>%
  ggplot() +
  geom_map(
    map = NZ,
    aes(long, lat, map_id = region),
    color = "white", fill = "grey50", linewidth = 0.1
  ) +
  geom_point(data=master_df %>%
              left_join(., metadata %>%
                    select(lakeName, `Load calc possible`,`Country`,`Longitude (decimal degrees)`,
                           `Latitude (decimal degrees)`), by="lakeName") %>%
               rename(load_calc=`Load calc possible`) %>%
                filter(Country %in% c("New Zealand")),
             aes(x=`Longitude (decimal degrees)`,
                 y=`Latitude (decimal degrees)`,
                 fill=load_calc),
             shape=21, color="black", size=4, alpha=0.8)+
  scale_fill_manual(values=c("#56B4E9","#E69F00"))+
  coord_map("moll") +
  labs(x="Longitude",
       y="Latitude") + 
  theme(panel.grid.major=element_line("grey80"),
        plot.background = element_rect(color="black", size=2),
        panel.border = element_rect(color="grey80", fill=NA),
        panel.background = element_rect(fill="white"),
        legend.position = "none")

GLEON_NZ
ggsave(here("figures/MS/FigureS1.NZ.png"), width=2.15, height=2.5,units="in", dpi=600)


Asia <- world_map %>% filter(region %in% c("China","Taiwan","Laos","Vietnam",
                                           "Cambodia","Thailand","Mongolia","Bangladesh"))
GLEON_ASIA<-Asia %>%
  ggplot() +
  geom_map(
    map = Asia,
    aes(long, lat, map_id = region),
    color = "white", fill = "grey50", linewidth = 0.1
  ) +
  geom_point(data=master_df %>%
              left_join(., metadata %>%
                    select(lakeName, `Load calc possible`,`Country`,`Longitude (decimal degrees)`,
                           `Latitude (decimal degrees)`), by="lakeName") %>%
               rename(load_calc=`Load calc possible`) %>%
                filter(Country %in% c("China","Taiwan")),
             aes(x=`Longitude (decimal degrees)`,
                 y=`Latitude (decimal degrees)`,
                 fill=load_calc),
             shape=21, color="black", size=4, alpha=0.8)+
  scale_fill_manual(values=c("#56B4E9","#E69F00"))+
  coord_map("moll", xlim= c(105,125), ylim = c(20,40)) +
  labs(x="Longitude",
       y="Latitude") + 
  theme(panel.grid.major=element_line("grey80"),
        plot.background = element_rect(color="black", size=2),
        panel.border = element_rect(color="grey80", fill=NA),
        panel.background = element_rect(fill="white"),
        legend.position = "none")
GLEON_ASIA
ggsave(here("figures/MS/FigureS1.Asia.png"), width=2.15, height=2.5,units="in", dpi=600)



# ## Assemble plots
# bottom_panel <- ( GLEON_ASIA + GLEON_EURO + GLEON_NZ) +
#   plot_layout(
#     # widths = c(1,1,1,1),
#               # heights = c(2,2.1,1.2,1),
#               ncol=3)

# library(gridExtra)
# grid.arrange(patchworkGrob(bottom_panel), left = "Latitude", bottom = "Longitude")
# 
# strbottom_panel
# 
# GLEON_global_map_alt / ((GLEON_NA + GLEON_ASIA) / (GLEON_EURO + GLEON_NZ)) + plot_layout(heights=c(3,1))
# 
# ggsave(here("figures/MS/FigureS1.studymap.png"), width=8, height=8,units="in", dpi=600)



```

#### Figure S2. Daily metab

```{r, out.width = '100%',fig.height=15}

## Plot A - boxplots

GPP_distribution<-bella_metab_summary %>% 
  left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  ggplot(aes(x=reorder(lakeName, meanGPP, FUN=mean), y=log10(GPP50), label=lakeName))+
  geom_point(shape=21,size=3)+
  # geom_point(aes(y=GPP50, x=lakeName))+
  geom_boxplot(aes(ymin = log10(GPP5),
                   lower = log10(GPP25),
                   middle = log10(GPP50),
                   upper = log10(GPP75),
                   ymax = log10(GPP95)),
               stat='identity')+
  ylab(expression(paste('log'[10]*' GPP (mg C m'^-2,' day'^-1,')')))+
  theme_pubr(base_size = 6,
             x.text.angle = 45)+
  theme(
        legend.position="none",
        # axis.title.y=element_text(size=6),
        # axis.text.y=element_text(size=24),
        axis.title.x=element_blank(),
        plot.margin=margin(0.5,0,0.5,0.5, "cm"),
        axis.ticks.x=element_blank())
    # scale_y_log10()
    # geom_label_repel() 
GPP_distribution

# ggsave(here("figures/GLEON/GPP_dist.png"), width=11, height=8,units="in", dpi=600)



## Plot B - daily estimates used to compute the median value

bella_metab_withMetaData_extendedSummer <- bind_rows(bella_metab_withMetaData_northern_extendedSummer, bella_metab_withMetaData_southern_extendedSummer, YYL)



daily_plot <- bella_metab_withMetaData_extendedSummer %>%
  group_by(lakeName)%>%
  mutate(n=n(),
         GPP_mgCm2 = na_if(GPP_mgCm2, GPP_mgCm2<1),
         GPP_gCm2 = GPP_mgCm2/10, 
         n_NAs=sum(is.na(GPP_mgCm2)),
         perc_missing_days=(n_NAs/153)*100) %>%
  filter(!perc_missing_days>50)%>%
  filter(season=="summer") %>%
  filter(lakeName %in% c(master_df$lakeName)) %>%
  ggplot(aes(x=solarDay,y=GPP_mgCm2, group=lakeName))+
  geom_point(size=1.5, shape=21, fill="grey50", alpha=0.5)+
  # scale_fill_manual(values=c("white","forestgreen"),
  #                   
  #                   labels=c("excluded","included"))+
  ylab(expression(paste('GPP (mg C m'^-2,' day'^-1,')')))+
  xlab("Date")+
  # clean_theme()+
  # theme_pubclean(base_size=8)+
  # theme_classic2(base_size=8)+
  theme_pubr(base_size = 6,
             x.text.angle = 45)+
  # labs_pubr(base_size=8)+
  theme(plot.margin=margin(0,0.5,0.5,0.5, "cm"),
        strip.text = element_text(size=6, face="bold"),
        strip.background = element_rect(fill="white"))+
  facet_wrap(~lakeName, scales="free", ncol=8)+
  scale_x_date(date_breaks = "4 weeks", date_labels = "%m-%d",  breaks = breaks_pretty(3)) 
daily_plot


# ggsave(here("figures/MS/FigureS2.dailyGPPincluded.png"), width=10, height=6,units="in", dpi=600)


## Export both 
GPP_distribution /
  daily_plot + plot_layout(heights = c(1, 4))

ggsave(here("figures/MS/FigureS2.boxplot_and_dailyGPP.png"), width=11, height=8.5,units="in", dpi=600)


```

#### Figure S3. Measured vs. modeled loads

```{r, out.width = '100%',fig.height=15}
#Make a dataframe of RMSEs
## RMSEs
DOC_vals <- master_df %>% filter(lakeName %in% observed_loads_kelly$lakeName) %>% drop_na(DOC_gm3_mod) %>% select(DOC_gm3_obs, DOC_gm3_mod)
actual_DOCin <- DOC_vals$DOC_gm3_obs
pred_DOCin <- DOC_vals$DOC_gm3_mod
Metrics::rmse(actual_DOCin,pred_DOCin)

TP_vals <- master_df %>% filter(lakeName %in% observed_loads_kelly$lakeName) %>% drop_na(TP_mgm3_mod) %>% select(TP_mgm3_obs, TP_mgm3_mod)
actual_TPin <- TP_vals$TP_mgm3_obs
pred_TPin <- TP_vals$TP_mgm3_mod
Metrics::rmse(actual_TPin,pred_TPin)

TP_vals_loads <- master_df %>% filter(lakeName %in% observed_loads_kelly$lakeName) %>% drop_na(TP_load_modeled) %>% select(TP_load_measured, TP_load_modeled)
actual_TPload <- TP_vals_loads$TP_load_measured
pred_TPload <- TP_vals_loads$TP_load_modeled
Metrics::rmse(actual_TPload,pred_TPload)

DOC_vals_loads <- master_df %>% filter(lakeName %in% observed_loads_kelly$lakeName) %>% drop_na(DOC_load_modeled) %>% select(DOC_load_measured, DOC_load_modeled)
actual_DOCload <- DOC_vals_loads$DOC_load_measured
pred_DOCload <- DOC_vals_loads$DOC_load_modeled
Metrics::rmse(actual_DOCload,pred_DOCload)


DOCin_lm <- lm(DOC_gm3_mod ~ DOC_gm3_obs, data=DOC_vals)
summary(DOCin_lm)

TPin_lm <- lm(TP_mgm3_mod ~ TP_mgm3_obs, data=TP_vals)
summary(TPin_lm)

#Make a little dataframe for plotting RMSEs 
names <- c("DOCin","TPin","DOCload","TPload")
RMSEs <- c(Metrics::rmse(actual_DOCin,pred_DOCin),Metrics::rmse(actual_TPin,pred_TPin),
           Metrics::rmse(actual_DOCload,pred_DOCload),Metrics::rmse(actual_TPload,pred_TPload))
RMSEs <- data.frame(names, RMSEs) %>%
    mutate(x="left",
         y="top")


my_breaks = c(1,3,10,30)
A<-master_df %>%
  mutate(
    DOC_gm3_obs = replace(DOC_gm3_obs, lakeName=="EastLong", 35.2), #replace with values from gridSearchInput file
    TP_mgm3_obs = replace(TP_mgm3_obs, lakeName=="EastLong", 48.9)) %>%
  ggplot(aes(x=(DOC_gm3_obs),y=(DOC_gm3_mod))) +
  geom_point(shape=21, alpha=0.5, fill="black", size=2.2)+
  geom_abline(intercept=0, slope=1)+
  theme_pubr(base_size = 12, border=TRUE)+
  labs(x=expression(paste('Measured inflow DOC g m'^-3,)),
       y=expression(paste('Modeled inflow DOC g m'^-3,)))+
  scale_x_log10(labels = my_breaks,
                breaks = my_breaks,
                limits = c(1,40))+
  scale_y_log10(labels = my_breaks,
                breaks = my_breaks,
                limits = c(1,40))+
  ggpp::geom_label_npc(data = RMSEs %>% filter(names=="DOCin"),
                 aes(npcx = x, npcy = y, label = paste("RMSE =",round(RMSEs,1))),
                 size=3, alpha=0.5)

my_breaks = c(10,30,100,300)
B<-master_df %>%
  mutate(
    DOC_gm3_obs = replace(DOC_gm3_obs, lakeName=="EastLong", 35.2), #replace with values from gridSearchInput file
    TP_mgm3_obs = replace(TP_mgm3_obs, lakeName=="EastLong", 48.9)) %>%
  ggplot(aes(x=(TP_mgm3_obs),y=(TP_mgm3_mod))) +
  geom_point(shape=21, alpha=0.5, fill="black", size=2.2)+
  geom_abline(intercept=0, slope=1)+
  theme_pubr(base_size = 12, border=TRUE)+
  labs(x=expression(paste('Measured inflow TP mg m'^-3,)),
       y=expression(paste('Modeled inflow TP mg m'^-3,)))+
  scale_x_log10(labels = my_breaks,
                breaks = my_breaks,
                limits = c(5,300))+
  scale_y_log10(labels = my_breaks,
                breaks = my_breaks,
                limits = c(5,300))+
  ggpp::geom_label_npc(data = RMSEs %>% filter(names=="TPin"),
                 aes(npcx = x, npcy = y, label = paste("RMSE =",round(RMSEs,1))),
                 size=3, alpha=0.5)

## Loads instead of concentrations
## Looks even better TBH.
my_breaks = c(1,10,100,1000,10000,100000)
tf<-trans_format("log10", math_format(10^.x))
C<-master_df %>%
  ggplot(aes(x=(DOC_load_measured),y=(DOC_load_modeled))) +
  geom_point(shape=21, alpha=0.5, fill="black", size=2.2)+
  geom_abline(intercept=0, slope=1)+
  geom_abline(intercept=0, slope=1)+
  theme_pubr(base_size = 12, border=TRUE)+
  labs(x=expression(paste('Measured DOC load kg day'^-1)),
       y=expression(paste('Modeled DOC load kg day'^-1)))+
  scale_x_log10(labels = tf(my_breaks),
                breaks = my_breaks,
                limits = c(1,100000))+
  scale_y_log10(labels =  tf(my_breaks),
                breaks = my_breaks,
                limits = c(1,100000))+
  ggpp::geom_label_npc(data = RMSEs %>% filter(names=="DOCload"),
                 aes(npcx = x, npcy = y, label = paste("RMSE =",round(RMSEs,1))),
                 size=3, alpha=0.5)

my_breaks = c(0.001,0.1,10,1000)
tf<-trans_format("log10", math_format(10^.x))
D<-master_df %>%
  ggplot(aes(x=(TP_load_measured),y=(TP_load_modeled))) +
  geom_point(shape=21, alpha=0.5, fill="black", size=2.2)+
  geom_abline(intercept=0, slope=1)+
  geom_abline(intercept=0, slope=1)+
  theme_pubr(base_size = 12, border=TRUE)+
  labs(x=expression(paste('Measured TP load kg day'^-1)),
       y=expression(paste('Modeled TP load kg day'^-1)))+
  scale_x_log10(labels = tf(my_breaks),
                breaks = my_breaks,
                limits = c(0.001,1000))+
  scale_y_log10(labels =  tf(my_breaks),
                breaks = my_breaks,
                limits = c(0.001,1000))+
  ggpp::geom_label_npc(data = RMSEs %>% filter(names=="TPload"),
                 aes(npcx = x, npcy = y, label = paste("RMSE =",round(RMSEs,1))),
                 size=3, alpha=0.5)

(A+B)/(C+D)+plot_annotation(tag_levels = 'A') 
# + plot_annotation(title = gg_title)
#   
# 
ggsave(here("figures/MS/FigS4.modeled_vs_measured_loads_18lakes.png"), dpi=600, width=8,height=7, units="in")
# 
#




#Compare the C:P
gg_title = str_wrap("modeled values are from preliminary Metropolis Results (wide range)", width=40)
 

E<-master_df %>%
  ggplot(aes(x=DOC_gm3_obs/TP_mgm3_obs,y=DOC_gm3_mod/TP_mgm3_mod)) +
  # ggplot(aes(x=DOC_load_measured/TP_load_measured*1000,y=DOC_load_modeled/TP_load_modeled*1000)) +
  geom_point()+
  geom_abline(intercept=0, slope=1)+
  theme_few()+
  geom_text_repel(aes(label = lakeName), size = 3, max.overlaps=2)+
  labs(x="Measured DOC:TP",
       y="Modeled DOC:TP")
E + plot_annotation(title = gg_title)

ggsave(here("figures/modeled_vs_measured_CtoP_18lakes.png"), dpi=600, width=5,height=5, units="in")

```

#### Figure S4. Pred vs Obs params for 18 calibration lakes using original Kelly model estimates

```{r}

predVals<-read.csv(here("data/preliminaryMetropolisResults/KellyParameter_outputWobservedloads.csv")) 
obsVals<-read.csv(here("data/preliminaryMetropolisResults/gridSearchInput_wphyto4bella.csv")) 
# predVals_outOfSample<-read.csv(here("data/preliminaryMetropolisResults/ExtendedRangeParameter_outputWestimatedloads.csv")) 
# obsVals_outOfSample<-read.csv(here("data/preliminaryMetropolisResults/gridSearchInput_unmodeledInfoLakes4bella.csv")) 

compareVals<-left_join(predVals,obsVals %>%
                         select(lakeName, TP, DOC, meanzMix, medianGPP, meanGPP, phytoplanktonCarbon_assumed50C2Chl)) %>%
  mutate(set="calibration")
# compareVals2 <- left_join(predVals_outOfSample,obsVals_outOfSample %>%
                            # select(lakeName, TP, DOC, meanzMix, medianGPP, meanGPP)) %>%
  # mutate(set="modeled") %>%
  # filter(!lakeName %in% compareVals$lakeName)

# all_compareVals <- bind_rows(compareVals,
                             # compareVals2)

## This version includes only the calibration set (18 lakes)
A <- compareVals %>%
  ggplot(aes(y=TPHat, x=TP))+
  geom_point(alpha=0.7, size=2.5)+
  theme_few()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean lake TP (mg m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled lake TP (mg m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))
  # scale_y_continuous(trans = log10_trans(),
  #   breaks = trans_breaks("log10", function(x) 10^x),
  #   labels = trans_format("log10", math_format(10^.x)),
  #   name = expression(paste('Modeled lake TP (mg m'^-3,')')))+
  # scale_x_continuous(trans = log10_trans(),
  #   breaks = trans_breaks("log10", function(x) 10^x),
  #   labels = trans_format("log10", math_format(10^.x)),
  #   name = expression(paste('Mean lake TP (mg m'^-3,')')))



B  <- compareVals %>%
  ggplot(aes(y=CHat, x=DOC))+
  geom_point(alpha=0.7, size=2.5)+
  theme_few()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean lake DOC (g m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled lake DOC (g m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))
  # scale_y_continuous(trans = log10_trans(),
  #   breaks = trans_breaks("log10", function(x) 10^x),
  #   labels = trans_format("log10", math_format(10^.x)),
  #   name = expression(paste('Modeled lake DOC (g m'^-3,')')))+
  # scale_x_continuous(trans = log10_trans(),
  #   breaks = trans_breaks("log10", function(x) 10^x),
  #   labels = trans_format("log10", math_format(10^.x)),
  #   name = expression(paste('Mean lake DOC (g m'^-3,')')))

C  <- compareVals %>%
  ggplot(aes(y=zMixHat, x=meanzMix))+
  geom_point(alpha=0.7, size=2.5)+
  theme_few()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean z'[mix],' (m)')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled z'[mix],' (m)')),labels = scales::comma, breaks_log(n = 5, base = 10))
  # scale_y_continuous(trans = log10_trans(),
  #   breaks = trans_breaks("log10", function(x) 10^x),
  #   labels = trans_format("log10", math_format(10^.x)),
  #   name = expression(paste('Modeled z'[mix],' (m)')))+
  # scale_x_continuous(trans = log10_trans(),
  #   breaks = trans_breaks("log10", function(x) 10^x),
  #   labels = trans_format("log10", math_format(10^.x)),
  #   name = expression(paste('Mean z'[mix],' (m)')))

D  <-
  compareVals %>%
  ggplot(aes(y=GPPHat, x=meanGPP))+
  geom_point(alpha=0.7, size=2.5)+
  theme_few()+
  geom_abline(intercept=0, slope=1)+
  labs(x=expression(paste('Mean GPP (mg C m'^-2,' day'^-1,')')),
       y=expression(paste('Modeled GPP (mg C m'^-2,' day'^-1,')')))

E  <-
  compareVals %>%
  ggplot(aes(y=AHat, x=phytoplanktonCarbon_assumed50C2Chl))+
  geom_point(alpha=0.7, size=2.5)+
  theme_few()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean algal biomass')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled algal biomass')),labels = scales::comma, breaks_log(n = 5, base = 10))


(A+B)/(C+D) + plot_annotation(tag_levels = 'A',
                              title="Model predictions using original parameters from Kelly et al. 2018")

ggsave(here("figures/MS/FigS4.modeled_vs_measured_18lakes_originalKelly.png"), dpi=600, width=8,height=8, units="in")


## This version includes all of the lakes --- though note in the original Metropolis run I only got 49 lakes back from Stuart. Need to look into why that is. 
A <- all_compareVals %>%
  ggplot(aes(y=TPHat, x=TP, fill=set))+
  geom_point(alpha=0.7, size=2.5, shape=21)+
  theme_pubr()+
  colorblindr::scale_fill_OkabeIto()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean lake TP (mg m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled lake TP (mg m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))


B  <- all_compareVals %>%
  ggplot(aes(y=CHat, x=DOC, fill=set))+
  geom_point(alpha=0.7, size=2.5, shape=21)+
  theme_pubr()+
  colorblindr::scale_fill_OkabeIto()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean lake DOC (g m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled lake DOC (g m'^-3,')')),labels = scales::comma, breaks_log(n = 5, base = 10))


C  <- all_compareVals %>%
  ggplot(aes(y=zMixHat, x=meanzMix, fill=set))+
  geom_point(alpha=0.7, size=2.5, shape=21)+
  theme_pubr()+
  colorblindr::scale_fill_OkabeIto()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean z'[mix],' (m)')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled z'[mix],' (m)')),labels = scales::comma, breaks_log(n = 5, base = 10))


D  <-
  all_compareVals %>%
  ggplot(aes(y=GPPHat, x=meanGPP, fill=set))+
  geom_point(alpha=0.7, size=2.5, shape=21)+
  theme_pubr()+
  colorblindr::scale_fill_OkabeIto()+
  geom_abline(intercept=0, slope=1)+
  scale_x_log10(name = expression(paste('Mean GPP (mg C m'^-2,' day'^-1,')')),labels = scales::comma, breaks_log(n = 5, base = 10))+
  scale_y_log10(name = expression(paste('Modeled GPP (mg C m'^-2,' day'^-1,')')),labels = scales::comma, breaks_log(n = 5, base = 10))


combined <- (A+B)/(C+D) & theme(legend.position="bottom") 

combined +  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect")

ggsave(here("figures/MS/Fig2.modeled_vs_measured_TP_DOC_zMIX_GPP_allLakes.png"), dpi=600, width=8,height=8, units="in")


```

#### Figure S6. Parameter histograms

```{r}

#Histograms of all the parameters

gridSearchVals_wide<-read.csv(here("data/preliminaryMetropolisResults/goodChainBI_wideRange_longCRCrun.csv"),
                 stringsAsFactors = FALSE) %>%
  mutate(set="wide range")
gridSearchVals_constrained<-read.csv(here("data/preliminaryMetropolisResults/goodChainBI_NOTthin_litRange_longCRCrun.csv"),
                 stringsAsFactors = FALSE) %>%
  mutate(set="constrained range")
gridSearchVals <- bind_rows(gridSearchVals_wide,
                            gridSearchVals_constrained)

gridSearchVals %>%
  pivot_longer(-set, names_to = "parameter", values_to = "value") %>%
  ggplot(aes(x=exp(value), fill=set))+
  geom_histogram(bins=50)+
  geom_vline(aes(xintercept=value, group=parameter), optim_params %>% filter(set %in% c("wide range","constrained range")), color="maroon")+
  colorblindr::scale_fill_OkabeIto(order = c(7,8))+
  facet_wrap(parameter~set, scales="free", ncol=4, labeller = label_wrap_gen(multi_line=FALSE)) +
  theme_few()+
  # labs(title="Histograms of wide-range parameter set",
       # subtitle="Red line indicates maximum log likelihood values")+
  theme(axis.title.x=element_blank(),
        legend.position="none",
        plot.margin = margin(0,1,0,0, "cm"))
ggsave(here("figures/MS/FigureS5.histograms_parameter_maxlogliklihood.png"), dpi=600, width=11,height=8.5, units="in")

```

#.
#.
#.
#.
#. ARCHIVE
### Fig2- lake and stream nutrients

```{r}
FigX_lake_and_load_conc<-master_df %>%
  ggplot(aes(x=DOC_mgL, y=TP_ugL,
             # fill=DOC_mgL/(TP_ugL),
             ))+
  geom_point(shape=21, size=3, color="black", fill="grey50")+
  # geom_abline(intercept = 0, slope = 1)+
  ylab(expression('Mean lake TP ug L'^-1))+
  xlab(expression(paste('Mean lake DOC mg L'^-1,)))+
  # scale_fill_continuous_sequential(palette = "Lajolla", rev=FALSE,
  #                                  name="Lake DOC:TP\n(mass:mass)")+
  theme_classic()+
    theme(legend.position = c(0.02, 0.89),
        legend.justification = c("left", "bottom"),
        legend.box.just = "right",
        legend.title.align = 0,
        legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.background = element_blank(),
        legend.margin = margin(0, 0, 0, 0),
        legend.title = element_text(size=10))+

master_df %>%
  filter(!dataset=="NA")%>%
  ggplot(aes(x=log10(DOC_load_kg), y=log10(TP_load_kg),
             # fill=DOC_load_kgday_mean/(TP_load_kgday_mean*1000),
             shape=dataset))+
  geom_point( size=3, color="black", fill="grey20")+
  scale_shape_manual(values=c(21,25),
                     name="Inflow estimates:")+
  # geom_abline(intercept = 0, slope = 1)+
  ylab(expression('Mean TP load kg day'^-1))+
  xlab(expression(paste('Mean DOC load kg day'^-1,)))+
  # scale_fill_continuous_sequential(palette = "Lajolla", rev=FALSE,
  #                                  name="Load DOC:TP\n(mass:mass)")+
  # annotate(geom="text",x=0.0001, y=150, label="n=17",
  #             color="black")+
  theme_classic()+
    geom_text_repel(aes(label = lakeName), size = 3)+
  #Moves legend to inside of plot
  theme(legend.position = c(0.05, 0.79),
        legend.justification = c("left", "bottom"),
        legend.box.just = "right",
        legend.title.align = 0,
        legend.direction = "vertical",
        legend.box = "vertical",
        legend.background = element_blank(),
        legend.margin = margin(0, 0, 0, 0),
        legend.title = element_text(size=10)) +
 plot_annotation(tag_levels = 'A')

FigX_lake_and_load_conc
ggsave(here("results/draft MS figs/lakeAndLoadConcentrations.png"), width=10, height=5,units="in", dpi=600)



```
### Figure 5 - Obs vs pred

```{r}

#PREDICTED VS OBSERVED
A<-master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
    left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  filter(!dataset=='NA')%>%
  ggplot(aes(y=medianGPP, x=PPareal, fill=dataset, shape=inflows_YN))+
  geom_point(size=3, alpha=0.8)+
  theme_bw()+
  xlab(expression(paste('PRED. GPP (mg C m'^-2,' day'^-1,')')))+
  ylab(expression(paste('OBS. GPP (mg C m'^-2,' day'^-1,')')))+
  scale_fill_manual(values=c("#e63946","#457b9d"),
                    name="How were loads\ndeteremined?")+
  # scale_fill_manual(values=c("#457b9d","#457b9d"),
  #                   name="How were loads\ndeteremined?")+
  scale_shape_manual(values=c(21,25),
                     name="Does the lake\nhave surface inflow(s)?")+
  geom_text_repel(aes(label = lakeName), size = 3)+
  geom_abline(intercept = 0, slope = 1)+
  facet_grid(.~dataset)+
  theme(panel.spacing = unit(0, "lines"))+
  guides(fill = guide_legend(override.aes = list(shape = 21) ),
         shape = guide_legend(override.aes = list(fill = "black") ) )+
  xlim(c(0,10000))+ylim(c(0,10000))


#Predicted "equilibrium" DOC values are much lower than observed DOC? Why?
B<-master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
    left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  filter(!dataset=='NA')%>%  ggplot(aes(y=DOC_mgL, x=eqDOC, fill=dataset, shape=inflows_YN))+
  geom_point( size=3, alpha=0.8)+
  geom_abline(intercept = 0, slope = 1)+
  xlab(expression(paste('PRED. lake DOC mg L'^-1,)))+
  ylab(expression(paste('OBS. lake DOC mg L'^-1,)))+
  scale_fill_manual(values=c("#e63946","#457b9d"),
                    name="How were loads\ndeteremined?")+
  # scale_fill_manual(values=c("#457b9d","#457b9d"),
  #                   name="How were loads\ndeteremined?")+
  scale_shape_manual(values=c(21,25),
                     name="Does the lake\nhave surface inflow(s)?")+
    geom_text_repel(aes(label = lakeName), size = 3)+
  theme_bw()+
  facet_grid(.~dataset)+
  theme(panel.spacing = unit(0, "lines"))+
  guides(fill = guide_legend(override.aes = list(shape = 21) ),
         shape = guide_legend(override.aes = list(fill = "black") ) )+
  xlim(c(0,75))+ylim(c(0,75))


#Model tends to overpredict DOC, but it's especially bad for two lakes (EastLong, Vortsjarv)

#Relationship between equilibrium TP and actual in-lake TP
C<-master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
    left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  filter(!dataset=='NA')%>%  ggplot(aes(x=eqTP, y=TP_ugL, fill=dataset, shape=inflows_YN))+
  geom_point(size=3, alpha=0.8)+
  geom_abline(intercept = 0, slope = 1)+
  xlab(expression(paste('PRED. lake TP ug L'^-1,)))+
  ylab(expression(paste('OBS. lake TP ug L'^-1,)))+
  scale_fill_manual(values=c("#e63946","#457b9d"),
                    name="How were loads\ndeteremined?")+
  # scale_fill_manual(values=c("#457b9d","#457b9d"),
  #                   name="How were loads\ndeteremined?")+
  scale_shape_manual(values=c(21,25),
                     name="Does the lake\nhave surface inflow(s)?")+
  theme_bw()+
  geom_text_repel(aes(label = lakeName), size = 3)+
  facet_grid(.~dataset)+
  theme(panel.spacing = unit(0, "lines"))+
  guides(fill = guide_legend(override.aes = list(shape = 21) ),
         shape = guide_legend(override.aes = list(fill = "black") ) )+
  xlim(c(0,350))+ylim(c(0,350))



#What do the zMix estimates look like compared to actual mean zMix observed values?
D<-master_df %>%
  left_join(., preds_full,  by="lakeName") %>%
    left_join(., metadata %>%
              select(lakeName, inflows_YN), by="lakeName") %>%
  filter(!dataset=='NA')%>%  ggplot(aes(y=meanzMix, x=zMix_mod, fill=dataset, shape=inflows_YN))+
  geom_point(size=3, alpha=0.8)+
  geom_abline(intercept = 0, slope = 1)+
  scale_fill_manual(values=c("#e63946","#457b9d"),
                    name="How were loads\ndeteremined?")+
  # scale_fill_manual(values=c("#457b9d","#457b9d"),
  #                   name="How were loads\ndeteremined?")+
  scale_shape_manual(values=c(21,25),
                     name="Does the lake\nhave surface inflow(s)?")+
  xlab("PRED. zMix (m)")+
  ylab("OBS. zMix (m)")+
  theme_bw()+
  geom_text_repel(aes(label = lakeName), size = 3)+
  facet_grid(.~dataset)+
  theme(panel.spacing = unit(0, "lines"))+
  guides(fill = guide_legend(override.aes = list(shape = 21) ),
         shape = guide_legend(override.aes = list(fill = "black") ) )+
  xlim(c(0,10))+ylim(c(0,10))


(A+B)/(C+D)+
  plot_layout(guides = 'collect')+
  plot_annotation(tag_levels = 'A')
ggsave(here("results/draft MS figs/Fig5.predVSmeasuredGPP-DOC-TP-zMix.png"), width=9, height=6,units="in", dpi=600)


```
